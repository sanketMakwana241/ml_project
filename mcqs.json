[
    {
        "QuestionId": 1,
        "QuestionDesc": "What is the primary objective of Linear Regression?",
        "QuestionOptions": [
            "To classify data points",
            "To establish a linear relationship",
            "To cluster data",
            "To reduce dimensions"
        ],
        "QuestionAnswer": "To establish a linear relationship",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression aims to model the relationship between dependent and independent variables."
    },
    {
        "QuestionId": 2,
        "QuestionDesc": "Which of the following represents a Simple Linear Regression equation?",
        "QuestionOptions": [
            "y = mx + c",
            "y = \u03b20 + \u03b21x + \u03b5",
            "y = ax\u00b2 + bx + c",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b5",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The equation consists of an intercept (\u03b20), a coefficient (\u03b21), and an error term (\u03b5)."
    },
    {
        "QuestionId": 3,
        "QuestionDesc": "Which component in Linear Regression represents the predicted value when x=0?",
        "QuestionOptions": [
            "Slope (\u03b21)",
            "Intercept (\u03b20)",
            "Error Term (\u03b5)",
            "Residual"
        ],
        "QuestionAnswer": "Intercept (\u03b20)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The intercept is the expected value of the dependent variable when all independent variables are zero."
    },
    {
        "QuestionId": 4,
        "QuestionDesc": "Which of the following is an assumption of Linear Regression?",
        "QuestionOptions": [
            "Linearity",
            "Overfitting",
            "Underfitting",
            "Gradient Descent"
        ],
        "QuestionAnswer": "Linearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression assumes a linear relationship between independent and dependent variables."
    },
    {
        "QuestionId": 5,
        "QuestionDesc": "Which statistical method is used to minimize error in Linear Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Log Loss"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "OLS minimizes the sum of squared differences between actual and predicted values."
    },
    {
        "QuestionId": 6,
        "QuestionDesc": "What does R-squared represent in a Linear Regression model?",
        "QuestionOptions": [
            "The number of independent variables",
            "The accuracy of predictions",
            "The proportion of variance explained",
            "The correlation coefficient"
        ],
        "QuestionAnswer": "The proportion of variance explained",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "R-squared measures how well the independent variables explain the variance in the dependent variable."
    },
    {
        "QuestionId": 7,
        "QuestionDesc": "Which technique is used to evaluate the performance of a Linear Regression model?",
        "QuestionOptions": [
            "Confusion Matrix",
            "Precision-Recall Curve",
            "Mean Squared Error (MSE)",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 8,
        "QuestionDesc": "What happens if independent variables in Linear Regression are highly correlated?",
        "QuestionOptions": [
            "Overfitting",
            "Underfitting",
            "Multicollinearity",
            "Bias"
        ],
        "QuestionAnswer": "Multicollinearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Multicollinearity occurs when independent variables are highly correlated, affecting model interpretation."
    },
    {
        "QuestionId": 9,
        "QuestionDesc": "Which of the following is an application of Linear Regression?",
        "QuestionOptions": [
            "Spam Email Detection",
            "Predicting House Prices",
            "Clustering Customers",
            "Image Classification"
        ],
        "QuestionAnswer": "Predicting House Prices",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression is commonly used to predict continuous values like house prices."
    },
    {
        "QuestionId": 10,
        "QuestionDesc": "What effect do outliers have on Linear Regression models?",
        "QuestionOptions": [
            "No effect",
            "Improves model accuracy",
            "Increases model bias",
            "Can distort predictions"
        ],
        "QuestionAnswer": "Can distort predictions",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Outliers can significantly impact the fitted regression line, leading to inaccurate predictions."
    },
    {
        "QuestionId": 11,
        "QuestionDesc": "What is the primary objective of Linear Regression?",
        "QuestionOptions": [
            "To classify data points",
            "To establish a linear relationship",
            "To cluster data",
            "To reduce dimensions"
        ],
        "QuestionAnswer": "To establish a linear relationship",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression aims to model the relationship between dependent and independent variables."
    },
    {
        "QuestionId": 12,
        "QuestionDesc": "Which of the following represents a Simple Linear Regression equation?",
        "QuestionOptions": [
            "y = mx + c",
            "y = \u03b20 + \u03b21x + \u03b5",
            "y = ax\u00b2 + bx + c",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b5",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The equation consists of an intercept (\u03b20), a coefficient (\u03b21), and an error term (\u03b5)."
    },
    {
        "QuestionId": 13,
        "QuestionDesc": "Which component in Linear Regression represents the predicted value when x=0?",
        "QuestionOptions": [
            "Slope (\u03b21)",
            "Intercept (\u03b20)",
            "Error Term (\u03b5)",
            "Residual"
        ],
        "QuestionAnswer": "Intercept (\u03b20)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The intercept is the expected value of the dependent variable when all independent variables are zero."
    },
    {
        "QuestionId": 14,
        "QuestionDesc": "Which of the following is an assumption of Linear Regression?",
        "QuestionOptions": [
            "Linearity",
            "Overfitting",
            "Underfitting",
            "Gradient Descent"
        ],
        "QuestionAnswer": "Linearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression assumes a linear relationship between independent and dependent variables."
    },
    {
        "QuestionId": 15,
        "QuestionDesc": "Which statistical method is used to minimize error in Linear Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Log Loss"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "OLS minimizes the sum of squared differences between actual and predicted values."
    },
    {
        "QuestionId": 16,
        "QuestionDesc": "What does R-squared represent in a Linear Regression model?",
        "QuestionOptions": [
            "The number of independent variables",
            "The accuracy of predictions",
            "The proportion of variance explained",
            "The correlation coefficient"
        ],
        "QuestionAnswer": "The proportion of variance explained",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "R-squared measures how well the independent variables explain the variance in the dependent variable."
    },
    {
        "QuestionId": 17,
        "QuestionDesc": "Which technique is used to evaluate the performance of a Linear Regression model?",
        "QuestionOptions": [
            "Confusion Matrix",
            "Precision-Recall Curve",
            "Mean Squared Error (MSE)",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 18,
        "QuestionDesc": "What happens if independent variables in Linear Regression are highly correlated?",
        "QuestionOptions": [
            "Overfitting",
            "Underfitting",
            "Multicollinearity",
            "Bias"
        ],
        "QuestionAnswer": "Multicollinearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Multicollinearity occurs when independent variables are highly correlated, affecting model interpretation."
    },
    {
        "QuestionId": 19,
        "QuestionDesc": "Which of the following is an application of Linear Regression?",
        "QuestionOptions": [
            "Spam Email Detection",
            "Predicting House Prices",
            "Clustering Customers",
            "Image Classification"
        ],
        "QuestionAnswer": "Predicting House Prices",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression is commonly used to predict continuous values like house prices."
    },
    {
        "QuestionId": 20,
        "QuestionDesc": "What effect do outliers have on Linear Regression models?",
        "QuestionOptions": [
            "No effect",
            "Improves model accuracy",
            "Increases model bias",
            "Can distort predictions"
        ],
        "QuestionAnswer": "Can distort predictions",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Outliers can significantly impact the fitted regression line, leading to inaccurate predictions."
    },
    {
        "QuestionId": 21,
        "QuestionDesc": "What is the primary objective of Linear Regression?",
        "QuestionOptions": [
            "To classify data points",
            "To establish a linear relationship",
            "To cluster data",
            "To reduce dimensions"
        ],
        "QuestionAnswer": "To establish a linear relationship",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression aims to model the relationship between dependent and independent variables."
    },
    {
        "QuestionId": 22,
        "QuestionDesc": "Which of the following represents a Simple Linear Regression equation?",
        "QuestionOptions": [
            "y = mx + c",
            "y = \u03b20 + \u03b21x + \u03b5",
            "y = ax\u00b2 + bx + c",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b5",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The equation consists of an intercept (\u03b20), a coefficient (\u03b21), and an error term (\u03b5)."
    },
    {
        "QuestionId": 23,
        "QuestionDesc": "Which component in Linear Regression represents the predicted value when x=0?",
        "QuestionOptions": [
            "Slope (\u03b21)",
            "Intercept (\u03b20)",
            "Error Term (\u03b5)",
            "Residual"
        ],
        "QuestionAnswer": "Intercept (\u03b20)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The intercept is the expected value of the dependent variable when all independent variables are zero."
    },
    {
        "QuestionId": 24,
        "QuestionDesc": "Which of the following is an assumption of Linear Regression?",
        "QuestionOptions": [
            "Linearity",
            "Overfitting",
            "Underfitting",
            "Gradient Descent"
        ],
        "QuestionAnswer": "Linearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression assumes a linear relationship between independent and dependent variables."
    },
    {
        "QuestionId": 25,
        "QuestionDesc": "Which statistical method is used to minimize error in Linear Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Log Loss"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "OLS minimizes the sum of squared differences between actual and predicted values."
    },
    {
        "QuestionId": 26,
        "QuestionDesc": "What does R-squared represent in a Linear Regression model?",
        "QuestionOptions": [
            "The number of independent variables",
            "The accuracy of predictions",
            "The proportion of variance explained",
            "The correlation coefficient"
        ],
        "QuestionAnswer": "The proportion of variance explained",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "R-squared measures how well the independent variables explain the variance in the dependent variable."
    },
    {
        "QuestionId": 27,
        "QuestionDesc": "Which technique is used to evaluate the performance of a Linear Regression model?",
        "QuestionOptions": [
            "Confusion Matrix",
            "Precision-Recall Curve",
            "Mean Squared Error (MSE)",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 28,
        "QuestionDesc": "What happens if independent variables in Linear Regression are highly correlated?",
        "QuestionOptions": [
            "Overfitting",
            "Underfitting",
            "Multicollinearity",
            "Bias"
        ],
        "QuestionAnswer": "Multicollinearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Multicollinearity occurs when independent variables are highly correlated, affecting model interpretation."
    },
    {
        "QuestionId": 29,
        "QuestionDesc": "Which of the following is an application of Linear Regression?",
        "QuestionOptions": [
            "Spam Email Detection",
            "Predicting House Prices",
            "Clustering Customers",
            "Image Classification"
        ],
        "QuestionAnswer": "Predicting House Prices",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression is commonly used to predict continuous values like house prices."
    },
    {
        "QuestionId": 30,
        "QuestionDesc": "What effect do outliers have on Linear Regression models?",
        "QuestionOptions": [
            "No effect",
            "Improves model accuracy",
            "Increases model bias",
            "Can distort predictions"
        ],
        "QuestionAnswer": "Can distort predictions",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Outliers can significantly impact the fitted regression line, leading to inaccurate predictions."
    },
    {
        "QuestionId": 31,
        "QuestionDesc": "What is the primary objective of Linear Regression?",
        "QuestionOptions": [
            "To classify data points",
            "To establish a linear relationship",
            "To cluster data",
            "To reduce dimensions"
        ],
        "QuestionAnswer": "To establish a linear relationship",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression aims to model the relationship between dependent and independent variables."
    },
    {
        "QuestionId": 32,
        "QuestionDesc": "Which of the following represents a Simple Linear Regression equation?",
        "QuestionOptions": [
            "y = mx + c",
            "y = \u03b20 + \u03b21x + \u03b5",
            "y = ax\u00b2 + bx + c",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b5",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The equation consists of an intercept (\u03b20), a coefficient (\u03b21), and an error term (\u03b5)."
    },
    {
        "QuestionId": 33,
        "QuestionDesc": "Which component in Linear Regression represents the predicted value when x=0?",
        "QuestionOptions": [
            "Slope (\u03b21)",
            "Intercept (\u03b20)",
            "Error Term (\u03b5)",
            "Residual"
        ],
        "QuestionAnswer": "Intercept (\u03b20)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The intercept is the expected value of the dependent variable when all independent variables are zero."
    },
    {
        "QuestionId": 34,
        "QuestionDesc": "Which of the following is an assumption of Linear Regression?",
        "QuestionOptions": [
            "Linearity",
            "Overfitting",
            "Underfitting",
            "Gradient Descent"
        ],
        "QuestionAnswer": "Linearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression assumes a linear relationship between independent and dependent variables."
    },
    {
        "QuestionId": 35,
        "QuestionDesc": "Which statistical method is used to minimize error in Linear Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Log Loss"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "OLS minimizes the sum of squared differences between actual and predicted values."
    },
    {
        "QuestionId": 36,
        "QuestionDesc": "What does R-squared represent in a Linear Regression model?",
        "QuestionOptions": [
            "The number of independent variables",
            "The accuracy of predictions",
            "The proportion of variance explained",
            "The correlation coefficient"
        ],
        "QuestionAnswer": "The proportion of variance explained",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "R-squared measures how well the independent variables explain the variance in the dependent variable."
    },
    {
        "QuestionId": 37,
        "QuestionDesc": "Which technique is used to evaluate the performance of a Linear Regression model?",
        "QuestionOptions": [
            "Confusion Matrix",
            "Precision-Recall Curve",
            "Mean Squared Error (MSE)",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 38,
        "QuestionDesc": "What happens if independent variables in Linear Regression are highly correlated?",
        "QuestionOptions": [
            "Overfitting",
            "Underfitting",
            "Multicollinearity",
            "Bias"
        ],
        "QuestionAnswer": "Multicollinearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Multicollinearity occurs when independent variables are highly correlated, affecting model interpretation."
    },
    {
        "QuestionId": 39,
        "QuestionDesc": "Which of the following is an application of Linear Regression?",
        "QuestionOptions": [
            "Spam Email Detection",
            "Predicting House Prices",
            "Clustering Customers",
            "Image Classification"
        ],
        "QuestionAnswer": "Predicting House Prices",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression is commonly used to predict continuous values like house prices."
    },
    {
        "QuestionId": 40,
        "QuestionDesc": "What effect do outliers have on Linear Regression models?",
        "QuestionOptions": [
            "No effect",
            "Improves model accuracy",
            "Increases model bias",
            "Can distort predictions"
        ],
        "QuestionAnswer": "Can distort predictions",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Outliers can significantly impact the fitted regression line, leading to inaccurate predictions."
    },
    {
        "QuestionId": 41,
        "QuestionDesc": "What is the primary objective of Linear Regression?",
        "QuestionOptions": [
            "To classify data points",
            "To establish a linear relationship",
            "To cluster data",
            "To reduce dimensions"
        ],
        "QuestionAnswer": "To establish a linear relationship",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression aims to model the relationship between dependent and independent variables."
    },
    {
        "QuestionId": 42,
        "QuestionDesc": "Which of the following represents a Simple Linear Regression equation?",
        "QuestionOptions": [
            "y = mx + c",
            "y = \u03b20 + \u03b21x + \u03b5",
            "y = ax\u00b2 + bx + c",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b5",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The equation consists of an intercept (\u03b20), a coefficient (\u03b21), and an error term (\u03b5)."
    },
    {
        "QuestionId": 43,
        "QuestionDesc": "Which component in Linear Regression represents the predicted value when x=0?",
        "QuestionOptions": [
            "Slope (\u03b21)",
            "Intercept (\u03b20)",
            "Error Term (\u03b5)",
            "Residual"
        ],
        "QuestionAnswer": "Intercept (\u03b20)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The intercept is the expected value of the dependent variable when all independent variables are zero."
    },
    {
        "QuestionId": 44,
        "QuestionDesc": "Which of the following is an assumption of Linear Regression?",
        "QuestionOptions": [
            "Linearity",
            "Overfitting",
            "Underfitting",
            "Gradient Descent"
        ],
        "QuestionAnswer": "Linearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression assumes a linear relationship between independent and dependent variables."
    },
    {
        "QuestionId": 45,
        "QuestionDesc": "Which statistical method is used to minimize error in Linear Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Log Loss"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "OLS minimizes the sum of squared differences between actual and predicted values."
    },
    {
        "QuestionId": 46,
        "QuestionDesc": "What does R-squared represent in a Linear Regression model?",
        "QuestionOptions": [
            "The number of independent variables",
            "The accuracy of predictions",
            "The proportion of variance explained",
            "The correlation coefficient"
        ],
        "QuestionAnswer": "The proportion of variance explained",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "R-squared measures how well the independent variables explain the variance in the dependent variable."
    },
    {
        "QuestionId": 47,
        "QuestionDesc": "Which technique is used to evaluate the performance of a Linear Regression model?",
        "QuestionOptions": [
            "Confusion Matrix",
            "Precision-Recall Curve",
            "Mean Squared Error (MSE)",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 48,
        "QuestionDesc": "What happens if independent variables in Linear Regression are highly correlated?",
        "QuestionOptions": [
            "Overfitting",
            "Underfitting",
            "Multicollinearity",
            "Bias"
        ],
        "QuestionAnswer": "Multicollinearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Multicollinearity occurs when independent variables are highly correlated, affecting model interpretation."
    },
    {
        "QuestionId": 49,
        "QuestionDesc": "Which of the following is an application of Linear Regression?",
        "QuestionOptions": [
            "Spam Email Detection",
            "Predicting House Prices",
            "Clustering Customers",
            "Image Classification"
        ],
        "QuestionAnswer": "Predicting House Prices",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression is commonly used to predict continuous values like house prices."
    },
    {
        "QuestionId": 50,
        "QuestionDesc": "What effect do outliers have on Linear Regression models?",
        "QuestionOptions": [
            "No effect",
            "Improves model accuracy",
            "Increases model bias",
            "Can distort predictions"
        ],
        "QuestionAnswer": "Can distort predictions",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Outliers can significantly impact the fitted regression line, leading to inaccurate predictions."
    },
    {
        "QuestionId": 51,
        "QuestionDesc": "What is the primary objective of Linear Regression?",
        "QuestionOptions": [
            "To classify data points",
            "To establish a linear relationship",
            "To cluster data",
            "To reduce dimensions"
        ],
        "QuestionAnswer": "To establish a linear relationship",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression aims to model the relationship between dependent and independent variables."
    },
    {
        "QuestionId": 52,
        "QuestionDesc": "Which of the following represents a Simple Linear Regression equation?",
        "QuestionOptions": [
            "y = mx + c",
            "y = \u03b20 + \u03b21x + \u03b5",
            "y = ax\u00b2 + bx + c",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b5",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The equation consists of an intercept (\u03b20), a coefficient (\u03b21), and an error term (\u03b5)."
    },
    {
        "QuestionId": 53,
        "QuestionDesc": "Which component in Linear Regression represents the predicted value when x=0?",
        "QuestionOptions": [
            "Slope (\u03b21)",
            "Intercept (\u03b20)",
            "Error Term (\u03b5)",
            "Residual"
        ],
        "QuestionAnswer": "Intercept (\u03b20)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The intercept is the expected value of the dependent variable when all independent variables are zero."
    },
    {
        "QuestionId": 54,
        "QuestionDesc": "Which of the following is an assumption of Linear Regression?",
        "QuestionOptions": [
            "Linearity",
            "Overfitting",
            "Underfitting",
            "Gradient Descent"
        ],
        "QuestionAnswer": "Linearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression assumes a linear relationship between independent and dependent variables."
    },
    {
        "QuestionId": 55,
        "QuestionDesc": "Which statistical method is used to minimize error in Linear Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Log Loss"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "OLS minimizes the sum of squared differences between actual and predicted values."
    },
    {
        "QuestionId": 56,
        "QuestionDesc": "What does R-squared represent in a Linear Regression model?",
        "QuestionOptions": [
            "The number of independent variables",
            "The accuracy of predictions",
            "The proportion of variance explained",
            "The correlation coefficient"
        ],
        "QuestionAnswer": "The proportion of variance explained",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "R-squared measures how well the independent variables explain the variance in the dependent variable."
    },
    {
        "QuestionId": 57,
        "QuestionDesc": "Which technique is used to evaluate the performance of a Linear Regression model?",
        "QuestionOptions": [
            "Confusion Matrix",
            "Precision-Recall Curve",
            "Mean Squared Error (MSE)",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 58,
        "QuestionDesc": "What happens if independent variables in Linear Regression are highly correlated?",
        "QuestionOptions": [
            "Overfitting",
            "Underfitting",
            "Multicollinearity",
            "Bias"
        ],
        "QuestionAnswer": "Multicollinearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Multicollinearity occurs when independent variables are highly correlated, affecting model interpretation."
    },
    {
        "QuestionId": 59,
        "QuestionDesc": "Which of the following is an application of Linear Regression?",
        "QuestionOptions": [
            "Spam Email Detection",
            "Predicting House Prices",
            "Clustering Customers",
            "Image Classification"
        ],
        "QuestionAnswer": "Predicting House Prices",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression is commonly used to predict continuous values like house prices."
    },
    {
        "QuestionId": 60,
        "QuestionDesc": "What effect do outliers have on Linear Regression models?",
        "QuestionOptions": [
            "No effect",
            "Improves model accuracy",
            "Increases model bias",
            "Can distort predictions"
        ],
        "QuestionAnswer": "Can distort predictions",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Outliers can significantly impact the fitted regression line, leading to inaccurate predictions."
    },
    {
        "QuestionId": 61,
        "QuestionDesc": "What is the primary objective of Linear Regression?",
        "QuestionOptions": [
            "To classify data points",
            "To establish a linear relationship",
            "To cluster data",
            "To reduce dimensions"
        ],
        "QuestionAnswer": "To establish a linear relationship",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression aims to model the relationship between dependent and independent variables."
    },
    {
        "QuestionId": 62,
        "QuestionDesc": "Which of the following represents a Simple Linear Regression equation?",
        "QuestionOptions": [
            "y = mx + c",
            "y = \u03b20 + \u03b21x + \u03b5",
            "y = ax\u00b2 + bx + c",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b5",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The equation consists of an intercept (\u03b20), a coefficient (\u03b21), and an error term (\u03b5)."
    },
    {
        "QuestionId": 63,
        "QuestionDesc": "Which component in Linear Regression represents the predicted value when x=0?",
        "QuestionOptions": [
            "Slope (\u03b21)",
            "Intercept (\u03b20)",
            "Error Term (\u03b5)",
            "Residual"
        ],
        "QuestionAnswer": "Intercept (\u03b20)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The intercept is the expected value of the dependent variable when all independent variables are zero."
    },
    {
        "QuestionId": 64,
        "QuestionDesc": "Which of the following is an assumption of Linear Regression?",
        "QuestionOptions": [
            "Linearity",
            "Overfitting",
            "Underfitting",
            "Gradient Descent"
        ],
        "QuestionAnswer": "Linearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression assumes a linear relationship between independent and dependent variables."
    },
    {
        "QuestionId": 65,
        "QuestionDesc": "Which statistical method is used to minimize error in Linear Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Log Loss"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "OLS minimizes the sum of squared differences between actual and predicted values."
    },
    {
        "QuestionId": 66,
        "QuestionDesc": "What does R-squared represent in a Linear Regression model?",
        "QuestionOptions": [
            "The number of independent variables",
            "The accuracy of predictions",
            "The proportion of variance explained",
            "The correlation coefficient"
        ],
        "QuestionAnswer": "The proportion of variance explained",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "R-squared measures how well the independent variables explain the variance in the dependent variable."
    },
    {
        "QuestionId": 67,
        "QuestionDesc": "Which technique is used to evaluate the performance of a Linear Regression model?",
        "QuestionOptions": [
            "Confusion Matrix",
            "Precision-Recall Curve",
            "Mean Squared Error (MSE)",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 68,
        "QuestionDesc": "What happens if independent variables in Linear Regression are highly correlated?",
        "QuestionOptions": [
            "Overfitting",
            "Underfitting",
            "Multicollinearity",
            "Bias"
        ],
        "QuestionAnswer": "Multicollinearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Multicollinearity occurs when independent variables are highly correlated, affecting model interpretation."
    },
    {
        "QuestionId": 69,
        "QuestionDesc": "Which of the following is an application of Linear Regression?",
        "QuestionOptions": [
            "Spam Email Detection",
            "Predicting House Prices",
            "Clustering Customers",
            "Image Classification"
        ],
        "QuestionAnswer": "Predicting House Prices",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression is commonly used to predict continuous values like house prices."
    },
    {
        "QuestionId": 70,
        "QuestionDesc": "What effect do outliers have on Linear Regression models?",
        "QuestionOptions": [
            "No effect",
            "Improves model accuracy",
            "Increases model bias",
            "Can distort predictions"
        ],
        "QuestionAnswer": "Can distort predictions",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Outliers can significantly impact the fitted regression line, leading to inaccurate predictions."
    },
    {
        "QuestionId": 71,
        "QuestionDesc": "What is the primary objective of Linear Regression?",
        "QuestionOptions": [
            "To classify data points",
            "To establish a linear relationship",
            "To cluster data",
            "To reduce dimensions"
        ],
        "QuestionAnswer": "To establish a linear relationship",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression aims to model the relationship between dependent and independent variables."
    },
    {
        "QuestionId": 72,
        "QuestionDesc": "Which of the following represents a Simple Linear Regression equation?",
        "QuestionOptions": [
            "y = mx + c",
            "y = \u03b20 + \u03b21x + \u03b5",
            "y = ax\u00b2 + bx + c",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b5",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The equation consists of an intercept (\u03b20), a coefficient (\u03b21), and an error term (\u03b5)."
    },
    {
        "QuestionId": 73,
        "QuestionDesc": "Which component in Linear Regression represents the predicted value when x=0?",
        "QuestionOptions": [
            "Slope (\u03b21)",
            "Intercept (\u03b20)",
            "Error Term (\u03b5)",
            "Residual"
        ],
        "QuestionAnswer": "Intercept (\u03b20)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The intercept is the expected value of the dependent variable when all independent variables are zero."
    },
    {
        "QuestionId": 74,
        "QuestionDesc": "Which of the following is an assumption of Linear Regression?",
        "QuestionOptions": [
            "Linearity",
            "Overfitting",
            "Underfitting",
            "Gradient Descent"
        ],
        "QuestionAnswer": "Linearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression assumes a linear relationship between independent and dependent variables."
    },
    {
        "QuestionId": 75,
        "QuestionDesc": "Which statistical method is used to minimize error in Linear Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Log Loss"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "OLS minimizes the sum of squared differences between actual and predicted values."
    },
    {
        "QuestionId": 76,
        "QuestionDesc": "What does R-squared represent in a Linear Regression model?",
        "QuestionOptions": [
            "The number of independent variables",
            "The accuracy of predictions",
            "The proportion of variance explained",
            "The correlation coefficient"
        ],
        "QuestionAnswer": "The proportion of variance explained",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "R-squared measures how well the independent variables explain the variance in the dependent variable."
    },
    {
        "QuestionId": 77,
        "QuestionDesc": "Which technique is used to evaluate the performance of a Linear Regression model?",
        "QuestionOptions": [
            "Confusion Matrix",
            "Precision-Recall Curve",
            "Mean Squared Error (MSE)",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 78,
        "QuestionDesc": "What happens if independent variables in Linear Regression are highly correlated?",
        "QuestionOptions": [
            "Overfitting",
            "Underfitting",
            "Multicollinearity",
            "Bias"
        ],
        "QuestionAnswer": "Multicollinearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Multicollinearity occurs when independent variables are highly correlated, affecting model interpretation."
    },
    {
        "QuestionId": 79,
        "QuestionDesc": "Which of the following is an application of Linear Regression?",
        "QuestionOptions": [
            "Spam Email Detection",
            "Predicting House Prices",
            "Clustering Customers",
            "Image Classification"
        ],
        "QuestionAnswer": "Predicting House Prices",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression is commonly used to predict continuous values like house prices."
    },
    {
        "QuestionId": 80,
        "QuestionDesc": "What effect do outliers have on Linear Regression models?",
        "QuestionOptions": [
            "No effect",
            "Improves model accuracy",
            "Increases model bias",
            "Can distort predictions"
        ],
        "QuestionAnswer": "Can distort predictions",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Outliers can significantly impact the fitted regression line, leading to inaccurate predictions."
    },
    {
        "QuestionId": 81,
        "QuestionDesc": "What is the primary objective of Linear Regression?",
        "QuestionOptions": [
            "To classify data points",
            "To establish a linear relationship",
            "To cluster data",
            "To reduce dimensions"
        ],
        "QuestionAnswer": "To establish a linear relationship",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression aims to model the relationship between dependent and independent variables."
    },
    {
        "QuestionId": 82,
        "QuestionDesc": "Which of the following represents a Simple Linear Regression equation?",
        "QuestionOptions": [
            "y = mx + c",
            "y = \u03b20 + \u03b21x + \u03b5",
            "y = ax\u00b2 + bx + c",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b5",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The equation consists of an intercept (\u03b20), a coefficient (\u03b21), and an error term (\u03b5)."
    },
    {
        "QuestionId": 83,
        "QuestionDesc": "Which component in Linear Regression represents the predicted value when x=0?",
        "QuestionOptions": [
            "Slope (\u03b21)",
            "Intercept (\u03b20)",
            "Error Term (\u03b5)",
            "Residual"
        ],
        "QuestionAnswer": "Intercept (\u03b20)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The intercept is the expected value of the dependent variable when all independent variables are zero."
    },
    {
        "QuestionId": 84,
        "QuestionDesc": "Which of the following is an assumption of Linear Regression?",
        "QuestionOptions": [
            "Linearity",
            "Overfitting",
            "Underfitting",
            "Gradient Descent"
        ],
        "QuestionAnswer": "Linearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression assumes a linear relationship between independent and dependent variables."
    },
    {
        "QuestionId": 85,
        "QuestionDesc": "Which statistical method is used to minimize error in Linear Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Log Loss"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "OLS minimizes the sum of squared differences between actual and predicted values."
    },
    {
        "QuestionId": 86,
        "QuestionDesc": "What does R-squared represent in a Linear Regression model?",
        "QuestionOptions": [
            "The number of independent variables",
            "The accuracy of predictions",
            "The proportion of variance explained",
            "The correlation coefficient"
        ],
        "QuestionAnswer": "The proportion of variance explained",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "R-squared measures how well the independent variables explain the variance in the dependent variable."
    },
    {
        "QuestionId": 87,
        "QuestionDesc": "Which technique is used to evaluate the performance of a Linear Regression model?",
        "QuestionOptions": [
            "Confusion Matrix",
            "Precision-Recall Curve",
            "Mean Squared Error (MSE)",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 88,
        "QuestionDesc": "What happens if independent variables in Linear Regression are highly correlated?",
        "QuestionOptions": [
            "Overfitting",
            "Underfitting",
            "Multicollinearity",
            "Bias"
        ],
        "QuestionAnswer": "Multicollinearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Multicollinearity occurs when independent variables are highly correlated, affecting model interpretation."
    },
    {
        "QuestionId": 89,
        "QuestionDesc": "Which of the following is an application of Linear Regression?",
        "QuestionOptions": [
            "Spam Email Detection",
            "Predicting House Prices",
            "Clustering Customers",
            "Image Classification"
        ],
        "QuestionAnswer": "Predicting House Prices",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression is commonly used to predict continuous values like house prices."
    },
    {
        "QuestionId": 90,
        "QuestionDesc": "What effect do outliers have on Linear Regression models?",
        "QuestionOptions": [
            "No effect",
            "Improves model accuracy",
            "Increases model bias",
            "Can distort predictions"
        ],
        "QuestionAnswer": "Can distort predictions",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Outliers can significantly impact the fitted regression line, leading to inaccurate predictions."
    },
    {
        "QuestionId": 91,
        "QuestionDesc": "What is the primary objective of Linear Regression?",
        "QuestionOptions": [
            "To classify data points",
            "To establish a linear relationship",
            "To cluster data",
            "To reduce dimensions"
        ],
        "QuestionAnswer": "To establish a linear relationship",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression aims to model the relationship between dependent and independent variables."
    },
    {
        "QuestionId": 92,
        "QuestionDesc": "Which of the following represents a Simple Linear Regression equation?",
        "QuestionOptions": [
            "y = mx + c",
            "y = \u03b20 + \u03b21x + \u03b5",
            "y = ax\u00b2 + bx + c",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b5",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The equation consists of an intercept (\u03b20), a coefficient (\u03b21), and an error term (\u03b5)."
    },
    {
        "QuestionId": 93,
        "QuestionDesc": "Which component in Linear Regression represents the predicted value when x=0?",
        "QuestionOptions": [
            "Slope (\u03b21)",
            "Intercept (\u03b20)",
            "Error Term (\u03b5)",
            "Residual"
        ],
        "QuestionAnswer": "Intercept (\u03b20)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "The intercept is the expected value of the dependent variable when all independent variables are zero."
    },
    {
        "QuestionId": 94,
        "QuestionDesc": "Which of the following is an assumption of Linear Regression?",
        "QuestionOptions": [
            "Linearity",
            "Overfitting",
            "Underfitting",
            "Gradient Descent"
        ],
        "QuestionAnswer": "Linearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression assumes a linear relationship between independent and dependent variables."
    },
    {
        "QuestionId": 95,
        "QuestionDesc": "Which statistical method is used to minimize error in Linear Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Log Loss"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "OLS minimizes the sum of squared differences between actual and predicted values."
    },
    {
        "QuestionId": 96,
        "QuestionDesc": "What does R-squared represent in a Linear Regression model?",
        "QuestionOptions": [
            "The number of independent variables",
            "The accuracy of predictions",
            "The proportion of variance explained",
            "The correlation coefficient"
        ],
        "QuestionAnswer": "The proportion of variance explained",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "R-squared measures how well the independent variables explain the variance in the dependent variable."
    },
    {
        "QuestionId": 97,
        "QuestionDesc": "Which technique is used to evaluate the performance of a Linear Regression model?",
        "QuestionOptions": [
            "Confusion Matrix",
            "Precision-Recall Curve",
            "Mean Squared Error (MSE)",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 98,
        "QuestionDesc": "What happens if independent variables in Linear Regression are highly correlated?",
        "QuestionOptions": [
            "Overfitting",
            "Underfitting",
            "Multicollinearity",
            "Bias"
        ],
        "QuestionAnswer": "Multicollinearity",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Multicollinearity occurs when independent variables are highly correlated, affecting model interpretation."
    },
    {
        "QuestionId": 99,
        "QuestionDesc": "Which of the following is an application of Linear Regression?",
        "QuestionOptions": [
            "Spam Email Detection",
            "Predicting House Prices",
            "Clustering Customers",
            "Image Classification"
        ],
        "QuestionAnswer": "Predicting House Prices",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Linear Regression is commonly used to predict continuous values like house prices."
    },
    {
        "QuestionId": 100,
        "QuestionDesc": "What effect do outliers have on Linear Regression models?",
        "QuestionOptions": [
            "No effect",
            "Improves model accuracy",
            "Increases model bias",
            "Can distort predictions"
        ],
        "QuestionAnswer": "Can distort predictions",
        "QuestionTopic": "Linear Regression",
        "QuestionReason": "Outliers can significantly impact the fitted regression line, leading to inaccurate predictions."
    },
    {
        "QuestionId": 101,
        "QuestionDesc": "What is the primary objective of Polynomial Regression?",
        "QuestionOptions": [
            "To model linear relationships",
            "To model non-linear relationships",
            "To reduce dimensionality",
            "To classify data"
        ],
        "QuestionAnswer": "To model non-linear relationships",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used when the relationship between independent and dependent variables is non-linear."
    },
    {
        "QuestionId": 102,
        "QuestionDesc": "Which of the following terms is unique to Polynomial Regression?",
        "QuestionOptions": [
            "Intercept (\u03b20)",
            "Slope (\u03b21)",
            "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
            "Error Term (\u03b5)"
        ],
        "QuestionAnswer": "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression includes polynomial terms like x\u00b2 and x\u00b3 to capture non-linear patterns."
    },
    {
        "QuestionId": 103,
        "QuestionDesc": "What is the equation for Quadratic Regression?",
        "QuestionOptions": [
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
            "y = \u03b20 + \u03b21x",
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2 + \u03b23x\u00b3",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Quadratic Regression is a second-degree Polynomial Regression model with an x\u00b2 term."
    },
    {
        "QuestionId": 104,
        "QuestionDesc": "Which assumption is specific to Polynomial Regression?",
        "QuestionOptions": [
            "The relationship between variables can be approximated using a polynomial function.",
            "The data must be normally distributed.",
            "There should be no missing values.",
            "All variables must be categorical."
        ],
        "QuestionAnswer": "The relationship between variables can be approximated using a polynomial function.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression assumes that a polynomial function can approximate the relationship between variables."
    },
    {
        "QuestionId": 105,
        "QuestionDesc": "Which method is commonly used to estimate coefficients in Polynomial Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Random Forest"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "OLS is used to minimize the sum of squared errors between actual and predicted values."
    },
    {
        "QuestionId": 106,
        "QuestionDesc": "Why is Polynomial Regression prone to overfitting?",
        "QuestionOptions": [
            "Because it has too few parameters",
            "Because it assumes a linear relationship",
            "Because high-degree polynomials can fit noise instead of patterns",
            "Because it ignores errors"
        ],
        "QuestionAnswer": "Because high-degree polynomials can fit noise instead of patterns",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Using high-degree polynomials can lead to overfitting, where the model captures noise instead of actual trends."
    },
    {
        "QuestionId": 107,
        "QuestionDesc": "What is the primary difference between Linear Regression and Polynomial Regression?",
        "QuestionOptions": [
            "Polynomial Regression includes polynomial terms to model non-linear relationships.",
            "Linear Regression is always more accurate.",
            "Polynomial Regression cannot be used for predictions.",
            "Polynomial Regression is only used for categorical data."
        ],
        "QuestionAnswer": "Polynomial Regression includes polynomial terms to model non-linear relationships.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression extends Linear Regression by including polynomial terms to capture non-linear patterns."
    },
    {
        "QuestionId": 108,
        "QuestionDesc": "Which of the following is an application of Polynomial Regression?",
        "QuestionOptions": [
            "Predicting continuous numerical values with non-linear trends",
            "Classifying emails as spam or not spam",
            "Grouping customers into segments",
            "Identifying faces in an image"
        ],
        "QuestionAnswer": "Predicting continuous numerical values with non-linear trends",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used in scenarios where the relationship between variables is non-linear."
    },
    {
        "QuestionId": 109,
        "QuestionDesc": "Which metric is used to evaluate the performance of a Polynomial Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Silhouette Score",
            "F1 Score",
            "Confusion Matrix"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 110,
        "QuestionDesc": "How can overfitting in Polynomial Regression be prevented?",
        "QuestionOptions": [
            "By increasing the polynomial degree",
            "By reducing data points",
            "By using regularization techniques like Ridge or Lasso",
            "By removing the intercept"
        ],
        "QuestionAnswer": "By using regularization techniques like Ridge or Lasso",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Regularization techniques like Ridge and Lasso help prevent overfitting by penalizing large coefficients."
    },
    {
        "QuestionId": 111,
        "QuestionDesc": "What is the primary objective of Polynomial Regression?",
        "QuestionOptions": [
            "To model linear relationships",
            "To model non-linear relationships",
            "To reduce dimensionality",
            "To classify data"
        ],
        "QuestionAnswer": "To model non-linear relationships",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used when the relationship between independent and dependent variables is non-linear."
    },
    {
        "QuestionId": 112,
        "QuestionDesc": "Which of the following terms is unique to Polynomial Regression?",
        "QuestionOptions": [
            "Intercept (\u03b20)",
            "Slope (\u03b21)",
            "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
            "Error Term (\u03b5)"
        ],
        "QuestionAnswer": "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression includes polynomial terms like x\u00b2 and x\u00b3 to capture non-linear patterns."
    },
    {
        "QuestionId": 113,
        "QuestionDesc": "What is the equation for Quadratic Regression?",
        "QuestionOptions": [
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
            "y = \u03b20 + \u03b21x",
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2 + \u03b23x\u00b3",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Quadratic Regression is a second-degree Polynomial Regression model with an x\u00b2 term."
    },
    {
        "QuestionId": 114,
        "QuestionDesc": "Which assumption is specific to Polynomial Regression?",
        "QuestionOptions": [
            "The relationship between variables can be approximated using a polynomial function.",
            "The data must be normally distributed.",
            "There should be no missing values.",
            "All variables must be categorical."
        ],
        "QuestionAnswer": "The relationship between variables can be approximated using a polynomial function.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression assumes that a polynomial function can approximate the relationship between variables."
    },
    {
        "QuestionId": 115,
        "QuestionDesc": "Which method is commonly used to estimate coefficients in Polynomial Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Random Forest"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "OLS is used to minimize the sum of squared errors between actual and predicted values."
    },
    {
        "QuestionId": 116,
        "QuestionDesc": "Why is Polynomial Regression prone to overfitting?",
        "QuestionOptions": [
            "Because it has too few parameters",
            "Because it assumes a linear relationship",
            "Because high-degree polynomials can fit noise instead of patterns",
            "Because it ignores errors"
        ],
        "QuestionAnswer": "Because high-degree polynomials can fit noise instead of patterns",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Using high-degree polynomials can lead to overfitting, where the model captures noise instead of actual trends."
    },
    {
        "QuestionId": 117,
        "QuestionDesc": "What is the primary difference between Linear Regression and Polynomial Regression?",
        "QuestionOptions": [
            "Polynomial Regression includes polynomial terms to model non-linear relationships.",
            "Linear Regression is always more accurate.",
            "Polynomial Regression cannot be used for predictions.",
            "Polynomial Regression is only used for categorical data."
        ],
        "QuestionAnswer": "Polynomial Regression includes polynomial terms to model non-linear relationships.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression extends Linear Regression by including polynomial terms to capture non-linear patterns."
    },
    {
        "QuestionId": 118,
        "QuestionDesc": "Which of the following is an application of Polynomial Regression?",
        "QuestionOptions": [
            "Predicting continuous numerical values with non-linear trends",
            "Classifying emails as spam or not spam",
            "Grouping customers into segments",
            "Identifying faces in an image"
        ],
        "QuestionAnswer": "Predicting continuous numerical values with non-linear trends",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used in scenarios where the relationship between variables is non-linear."
    },
    {
        "QuestionId": 119,
        "QuestionDesc": "Which metric is used to evaluate the performance of a Polynomial Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Silhouette Score",
            "F1 Score",
            "Confusion Matrix"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 120,
        "QuestionDesc": "How can overfitting in Polynomial Regression be prevented?",
        "QuestionOptions": [
            "By increasing the polynomial degree",
            "By reducing data points",
            "By using regularization techniques like Ridge or Lasso",
            "By removing the intercept"
        ],
        "QuestionAnswer": "By using regularization techniques like Ridge or Lasso",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Regularization techniques like Ridge and Lasso help prevent overfitting by penalizing large coefficients."
    },
    {
        "QuestionId": 121,
        "QuestionDesc": "What is the primary objective of Polynomial Regression?",
        "QuestionOptions": [
            "To model linear relationships",
            "To model non-linear relationships",
            "To reduce dimensionality",
            "To classify data"
        ],
        "QuestionAnswer": "To model non-linear relationships",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used when the relationship between independent and dependent variables is non-linear."
    },
    {
        "QuestionId": 122,
        "QuestionDesc": "Which of the following terms is unique to Polynomial Regression?",
        "QuestionOptions": [
            "Intercept (\u03b20)",
            "Slope (\u03b21)",
            "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
            "Error Term (\u03b5)"
        ],
        "QuestionAnswer": "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression includes polynomial terms like x\u00b2 and x\u00b3 to capture non-linear patterns."
    },
    {
        "QuestionId": 123,
        "QuestionDesc": "What is the equation for Quadratic Regression?",
        "QuestionOptions": [
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
            "y = \u03b20 + \u03b21x",
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2 + \u03b23x\u00b3",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Quadratic Regression is a second-degree Polynomial Regression model with an x\u00b2 term."
    },
    {
        "QuestionId": 124,
        "QuestionDesc": "Which assumption is specific to Polynomial Regression?",
        "QuestionOptions": [
            "The relationship between variables can be approximated using a polynomial function.",
            "The data must be normally distributed.",
            "There should be no missing values.",
            "All variables must be categorical."
        ],
        "QuestionAnswer": "The relationship between variables can be approximated using a polynomial function.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression assumes that a polynomial function can approximate the relationship between variables."
    },
    {
        "QuestionId": 125,
        "QuestionDesc": "Which method is commonly used to estimate coefficients in Polynomial Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Random Forest"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "OLS is used to minimize the sum of squared errors between actual and predicted values."
    },
    {
        "QuestionId": 126,
        "QuestionDesc": "Why is Polynomial Regression prone to overfitting?",
        "QuestionOptions": [
            "Because it has too few parameters",
            "Because it assumes a linear relationship",
            "Because high-degree polynomials can fit noise instead of patterns",
            "Because it ignores errors"
        ],
        "QuestionAnswer": "Because high-degree polynomials can fit noise instead of patterns",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Using high-degree polynomials can lead to overfitting, where the model captures noise instead of actual trends."
    },
    {
        "QuestionId": 127,
        "QuestionDesc": "What is the primary difference between Linear Regression and Polynomial Regression?",
        "QuestionOptions": [
            "Polynomial Regression includes polynomial terms to model non-linear relationships.",
            "Linear Regression is always more accurate.",
            "Polynomial Regression cannot be used for predictions.",
            "Polynomial Regression is only used for categorical data."
        ],
        "QuestionAnswer": "Polynomial Regression includes polynomial terms to model non-linear relationships.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression extends Linear Regression by including polynomial terms to capture non-linear patterns."
    },
    {
        "QuestionId": 128,
        "QuestionDesc": "Which of the following is an application of Polynomial Regression?",
        "QuestionOptions": [
            "Predicting continuous numerical values with non-linear trends",
            "Classifying emails as spam or not spam",
            "Grouping customers into segments",
            "Identifying faces in an image"
        ],
        "QuestionAnswer": "Predicting continuous numerical values with non-linear trends",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used in scenarios where the relationship between variables is non-linear."
    },
    {
        "QuestionId": 129,
        "QuestionDesc": "Which metric is used to evaluate the performance of a Polynomial Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Silhouette Score",
            "F1 Score",
            "Confusion Matrix"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 130,
        "QuestionDesc": "How can overfitting in Polynomial Regression be prevented?",
        "QuestionOptions": [
            "By increasing the polynomial degree",
            "By reducing data points",
            "By using regularization techniques like Ridge or Lasso",
            "By removing the intercept"
        ],
        "QuestionAnswer": "By using regularization techniques like Ridge or Lasso",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Regularization techniques like Ridge and Lasso help prevent overfitting by penalizing large coefficients."
    },
    {
        "QuestionId": 131,
        "QuestionDesc": "What is the primary objective of Polynomial Regression?",
        "QuestionOptions": [
            "To model linear relationships",
            "To model non-linear relationships",
            "To reduce dimensionality",
            "To classify data"
        ],
        "QuestionAnswer": "To model non-linear relationships",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used when the relationship between independent and dependent variables is non-linear."
    },
    {
        "QuestionId": 132,
        "QuestionDesc": "Which of the following terms is unique to Polynomial Regression?",
        "QuestionOptions": [
            "Intercept (\u03b20)",
            "Slope (\u03b21)",
            "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
            "Error Term (\u03b5)"
        ],
        "QuestionAnswer": "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression includes polynomial terms like x\u00b2 and x\u00b3 to capture non-linear patterns."
    },
    {
        "QuestionId": 133,
        "QuestionDesc": "What is the equation for Quadratic Regression?",
        "QuestionOptions": [
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
            "y = \u03b20 + \u03b21x",
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2 + \u03b23x\u00b3",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Quadratic Regression is a second-degree Polynomial Regression model with an x\u00b2 term."
    },
    {
        "QuestionId": 134,
        "QuestionDesc": "Which assumption is specific to Polynomial Regression?",
        "QuestionOptions": [
            "The relationship between variables can be approximated using a polynomial function.",
            "The data must be normally distributed.",
            "There should be no missing values.",
            "All variables must be categorical."
        ],
        "QuestionAnswer": "The relationship between variables can be approximated using a polynomial function.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression assumes that a polynomial function can approximate the relationship between variables."
    },
    {
        "QuestionId": 135,
        "QuestionDesc": "Which method is commonly used to estimate coefficients in Polynomial Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Random Forest"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "OLS is used to minimize the sum of squared errors between actual and predicted values."
    },
    {
        "QuestionId": 136,
        "QuestionDesc": "Why is Polynomial Regression prone to overfitting?",
        "QuestionOptions": [
            "Because it has too few parameters",
            "Because it assumes a linear relationship",
            "Because high-degree polynomials can fit noise instead of patterns",
            "Because it ignores errors"
        ],
        "QuestionAnswer": "Because high-degree polynomials can fit noise instead of patterns",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Using high-degree polynomials can lead to overfitting, where the model captures noise instead of actual trends."
    },
    {
        "QuestionId": 137,
        "QuestionDesc": "What is the primary difference between Linear Regression and Polynomial Regression?",
        "QuestionOptions": [
            "Polynomial Regression includes polynomial terms to model non-linear relationships.",
            "Linear Regression is always more accurate.",
            "Polynomial Regression cannot be used for predictions.",
            "Polynomial Regression is only used for categorical data."
        ],
        "QuestionAnswer": "Polynomial Regression includes polynomial terms to model non-linear relationships.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression extends Linear Regression by including polynomial terms to capture non-linear patterns."
    },
    {
        "QuestionId": 138,
        "QuestionDesc": "Which of the following is an application of Polynomial Regression?",
        "QuestionOptions": [
            "Predicting continuous numerical values with non-linear trends",
            "Classifying emails as spam or not spam",
            "Grouping customers into segments",
            "Identifying faces in an image"
        ],
        "QuestionAnswer": "Predicting continuous numerical values with non-linear trends",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used in scenarios where the relationship between variables is non-linear."
    },
    {
        "QuestionId": 139,
        "QuestionDesc": "Which metric is used to evaluate the performance of a Polynomial Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Silhouette Score",
            "F1 Score",
            "Confusion Matrix"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 140,
        "QuestionDesc": "How can overfitting in Polynomial Regression be prevented?",
        "QuestionOptions": [
            "By increasing the polynomial degree",
            "By reducing data points",
            "By using regularization techniques like Ridge or Lasso",
            "By removing the intercept"
        ],
        "QuestionAnswer": "By using regularization techniques like Ridge or Lasso",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Regularization techniques like Ridge and Lasso help prevent overfitting by penalizing large coefficients."
    },
    {
        "QuestionId": 141,
        "QuestionDesc": "What is the primary objective of Polynomial Regression?",
        "QuestionOptions": [
            "To model linear relationships",
            "To model non-linear relationships",
            "To reduce dimensionality",
            "To classify data"
        ],
        "QuestionAnswer": "To model non-linear relationships",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used when the relationship between independent and dependent variables is non-linear."
    },
    {
        "QuestionId": 142,
        "QuestionDesc": "Which of the following terms is unique to Polynomial Regression?",
        "QuestionOptions": [
            "Intercept (\u03b20)",
            "Slope (\u03b21)",
            "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
            "Error Term (\u03b5)"
        ],
        "QuestionAnswer": "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression includes polynomial terms like x\u00b2 and x\u00b3 to capture non-linear patterns."
    },
    {
        "QuestionId": 143,
        "QuestionDesc": "What is the equation for Quadratic Regression?",
        "QuestionOptions": [
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
            "y = \u03b20 + \u03b21x",
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2 + \u03b23x\u00b3",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Quadratic Regression is a second-degree Polynomial Regression model with an x\u00b2 term."
    },
    {
        "QuestionId": 144,
        "QuestionDesc": "Which assumption is specific to Polynomial Regression?",
        "QuestionOptions": [
            "The relationship between variables can be approximated using a polynomial function.",
            "The data must be normally distributed.",
            "There should be no missing values.",
            "All variables must be categorical."
        ],
        "QuestionAnswer": "The relationship between variables can be approximated using a polynomial function.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression assumes that a polynomial function can approximate the relationship between variables."
    },
    {
        "QuestionId": 145,
        "QuestionDesc": "Which method is commonly used to estimate coefficients in Polynomial Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Random Forest"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "OLS is used to minimize the sum of squared errors between actual and predicted values."
    },
    {
        "QuestionId": 146,
        "QuestionDesc": "Why is Polynomial Regression prone to overfitting?",
        "QuestionOptions": [
            "Because it has too few parameters",
            "Because it assumes a linear relationship",
            "Because high-degree polynomials can fit noise instead of patterns",
            "Because it ignores errors"
        ],
        "QuestionAnswer": "Because high-degree polynomials can fit noise instead of patterns",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Using high-degree polynomials can lead to overfitting, where the model captures noise instead of actual trends."
    },
    {
        "QuestionId": 147,
        "QuestionDesc": "What is the primary difference between Linear Regression and Polynomial Regression?",
        "QuestionOptions": [
            "Polynomial Regression includes polynomial terms to model non-linear relationships.",
            "Linear Regression is always more accurate.",
            "Polynomial Regression cannot be used for predictions.",
            "Polynomial Regression is only used for categorical data."
        ],
        "QuestionAnswer": "Polynomial Regression includes polynomial terms to model non-linear relationships.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression extends Linear Regression by including polynomial terms to capture non-linear patterns."
    },
    {
        "QuestionId": 148,
        "QuestionDesc": "Which of the following is an application of Polynomial Regression?",
        "QuestionOptions": [
            "Predicting continuous numerical values with non-linear trends",
            "Classifying emails as spam or not spam",
            "Grouping customers into segments",
            "Identifying faces in an image"
        ],
        "QuestionAnswer": "Predicting continuous numerical values with non-linear trends",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used in scenarios where the relationship between variables is non-linear."
    },
    {
        "QuestionId": 149,
        "QuestionDesc": "Which metric is used to evaluate the performance of a Polynomial Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Silhouette Score",
            "F1 Score",
            "Confusion Matrix"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 150,
        "QuestionDesc": "How can overfitting in Polynomial Regression be prevented?",
        "QuestionOptions": [
            "By increasing the polynomial degree",
            "By reducing data points",
            "By using regularization techniques like Ridge or Lasso",
            "By removing the intercept"
        ],
        "QuestionAnswer": "By using regularization techniques like Ridge or Lasso",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Regularization techniques like Ridge and Lasso help prevent overfitting by penalizing large coefficients."
    },
    {
        "QuestionId": 151,
        "QuestionDesc": "What is the primary objective of Polynomial Regression?",
        "QuestionOptions": [
            "To model linear relationships",
            "To model non-linear relationships",
            "To reduce dimensionality",
            "To classify data"
        ],
        "QuestionAnswer": "To model non-linear relationships",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used when the relationship between independent and dependent variables is non-linear."
    },
    {
        "QuestionId": 152,
        "QuestionDesc": "Which of the following terms is unique to Polynomial Regression?",
        "QuestionOptions": [
            "Intercept (\u03b20)",
            "Slope (\u03b21)",
            "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
            "Error Term (\u03b5)"
        ],
        "QuestionAnswer": "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression includes polynomial terms like x\u00b2 and x\u00b3 to capture non-linear patterns."
    },
    {
        "QuestionId": 153,
        "QuestionDesc": "What is the equation for Quadratic Regression?",
        "QuestionOptions": [
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
            "y = \u03b20 + \u03b21x",
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2 + \u03b23x\u00b3",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Quadratic Regression is a second-degree Polynomial Regression model with an x\u00b2 term."
    },
    {
        "QuestionId": 154,
        "QuestionDesc": "Which assumption is specific to Polynomial Regression?",
        "QuestionOptions": [
            "The relationship between variables can be approximated using a polynomial function.",
            "The data must be normally distributed.",
            "There should be no missing values.",
            "All variables must be categorical."
        ],
        "QuestionAnswer": "The relationship between variables can be approximated using a polynomial function.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression assumes that a polynomial function can approximate the relationship between variables."
    },
    {
        "QuestionId": 155,
        "QuestionDesc": "Which method is commonly used to estimate coefficients in Polynomial Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Random Forest"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "OLS is used to minimize the sum of squared errors between actual and predicted values."
    },
    {
        "QuestionId": 156,
        "QuestionDesc": "Why is Polynomial Regression prone to overfitting?",
        "QuestionOptions": [
            "Because it has too few parameters",
            "Because it assumes a linear relationship",
            "Because high-degree polynomials can fit noise instead of patterns",
            "Because it ignores errors"
        ],
        "QuestionAnswer": "Because high-degree polynomials can fit noise instead of patterns",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Using high-degree polynomials can lead to overfitting, where the model captures noise instead of actual trends."
    },
    {
        "QuestionId": 157,
        "QuestionDesc": "What is the primary difference between Linear Regression and Polynomial Regression?",
        "QuestionOptions": [
            "Polynomial Regression includes polynomial terms to model non-linear relationships.",
            "Linear Regression is always more accurate.",
            "Polynomial Regression cannot be used for predictions.",
            "Polynomial Regression is only used for categorical data."
        ],
        "QuestionAnswer": "Polynomial Regression includes polynomial terms to model non-linear relationships.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression extends Linear Regression by including polynomial terms to capture non-linear patterns."
    },
    {
        "QuestionId": 158,
        "QuestionDesc": "Which of the following is an application of Polynomial Regression?",
        "QuestionOptions": [
            "Predicting continuous numerical values with non-linear trends",
            "Classifying emails as spam or not spam",
            "Grouping customers into segments",
            "Identifying faces in an image"
        ],
        "QuestionAnswer": "Predicting continuous numerical values with non-linear trends",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used in scenarios where the relationship between variables is non-linear."
    },
    {
        "QuestionId": 159,
        "QuestionDesc": "Which metric is used to evaluate the performance of a Polynomial Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Silhouette Score",
            "F1 Score",
            "Confusion Matrix"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 160,
        "QuestionDesc": "How can overfitting in Polynomial Regression be prevented?",
        "QuestionOptions": [
            "By increasing the polynomial degree",
            "By reducing data points",
            "By using regularization techniques like Ridge or Lasso",
            "By removing the intercept"
        ],
        "QuestionAnswer": "By using regularization techniques like Ridge or Lasso",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Regularization techniques like Ridge and Lasso help prevent overfitting by penalizing large coefficients."
    },
    {
        "QuestionId": 161,
        "QuestionDesc": "What is the primary objective of Polynomial Regression?",
        "QuestionOptions": [
            "To model linear relationships",
            "To model non-linear relationships",
            "To reduce dimensionality",
            "To classify data"
        ],
        "QuestionAnswer": "To model non-linear relationships",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used when the relationship between independent and dependent variables is non-linear."
    },
    {
        "QuestionId": 162,
        "QuestionDesc": "Which of the following terms is unique to Polynomial Regression?",
        "QuestionOptions": [
            "Intercept (\u03b20)",
            "Slope (\u03b21)",
            "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
            "Error Term (\u03b5)"
        ],
        "QuestionAnswer": "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression includes polynomial terms like x\u00b2 and x\u00b3 to capture non-linear patterns."
    },
    {
        "QuestionId": 163,
        "QuestionDesc": "What is the equation for Quadratic Regression?",
        "QuestionOptions": [
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
            "y = \u03b20 + \u03b21x",
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2 + \u03b23x\u00b3",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Quadratic Regression is a second-degree Polynomial Regression model with an x\u00b2 term."
    },
    {
        "QuestionId": 164,
        "QuestionDesc": "Which assumption is specific to Polynomial Regression?",
        "QuestionOptions": [
            "The relationship between variables can be approximated using a polynomial function.",
            "The data must be normally distributed.",
            "There should be no missing values.",
            "All variables must be categorical."
        ],
        "QuestionAnswer": "The relationship between variables can be approximated using a polynomial function.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression assumes that a polynomial function can approximate the relationship between variables."
    },
    {
        "QuestionId": 165,
        "QuestionDesc": "Which method is commonly used to estimate coefficients in Polynomial Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Random Forest"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "OLS is used to minimize the sum of squared errors between actual and predicted values."
    },
    {
        "QuestionId": 166,
        "QuestionDesc": "Why is Polynomial Regression prone to overfitting?",
        "QuestionOptions": [
            "Because it has too few parameters",
            "Because it assumes a linear relationship",
            "Because high-degree polynomials can fit noise instead of patterns",
            "Because it ignores errors"
        ],
        "QuestionAnswer": "Because high-degree polynomials can fit noise instead of patterns",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Using high-degree polynomials can lead to overfitting, where the model captures noise instead of actual trends."
    },
    {
        "QuestionId": 167,
        "QuestionDesc": "What is the primary difference between Linear Regression and Polynomial Regression?",
        "QuestionOptions": [
            "Polynomial Regression includes polynomial terms to model non-linear relationships.",
            "Linear Regression is always more accurate.",
            "Polynomial Regression cannot be used for predictions.",
            "Polynomial Regression is only used for categorical data."
        ],
        "QuestionAnswer": "Polynomial Regression includes polynomial terms to model non-linear relationships.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression extends Linear Regression by including polynomial terms to capture non-linear patterns."
    },
    {
        "QuestionId": 168,
        "QuestionDesc": "Which of the following is an application of Polynomial Regression?",
        "QuestionOptions": [
            "Predicting continuous numerical values with non-linear trends",
            "Classifying emails as spam or not spam",
            "Grouping customers into segments",
            "Identifying faces in an image"
        ],
        "QuestionAnswer": "Predicting continuous numerical values with non-linear trends",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used in scenarios where the relationship between variables is non-linear."
    },
    {
        "QuestionId": 169,
        "QuestionDesc": "Which metric is used to evaluate the performance of a Polynomial Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Silhouette Score",
            "F1 Score",
            "Confusion Matrix"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 170,
        "QuestionDesc": "How can overfitting in Polynomial Regression be prevented?",
        "QuestionOptions": [
            "By increasing the polynomial degree",
            "By reducing data points",
            "By using regularization techniques like Ridge or Lasso",
            "By removing the intercept"
        ],
        "QuestionAnswer": "By using regularization techniques like Ridge or Lasso",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Regularization techniques like Ridge and Lasso help prevent overfitting by penalizing large coefficients."
    },
    {
        "QuestionId": 171,
        "QuestionDesc": "What is the primary objective of Polynomial Regression?",
        "QuestionOptions": [
            "To model linear relationships",
            "To model non-linear relationships",
            "To reduce dimensionality",
            "To classify data"
        ],
        "QuestionAnswer": "To model non-linear relationships",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used when the relationship between independent and dependent variables is non-linear."
    },
    {
        "QuestionId": 172,
        "QuestionDesc": "Which of the following terms is unique to Polynomial Regression?",
        "QuestionOptions": [
            "Intercept (\u03b20)",
            "Slope (\u03b21)",
            "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
            "Error Term (\u03b5)"
        ],
        "QuestionAnswer": "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression includes polynomial terms like x\u00b2 and x\u00b3 to capture non-linear patterns."
    },
    {
        "QuestionId": 173,
        "QuestionDesc": "What is the equation for Quadratic Regression?",
        "QuestionOptions": [
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
            "y = \u03b20 + \u03b21x",
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2 + \u03b23x\u00b3",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Quadratic Regression is a second-degree Polynomial Regression model with an x\u00b2 term."
    },
    {
        "QuestionId": 174,
        "QuestionDesc": "Which assumption is specific to Polynomial Regression?",
        "QuestionOptions": [
            "The relationship between variables can be approximated using a polynomial function.",
            "The data must be normally distributed.",
            "There should be no missing values.",
            "All variables must be categorical."
        ],
        "QuestionAnswer": "The relationship between variables can be approximated using a polynomial function.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression assumes that a polynomial function can approximate the relationship between variables."
    },
    {
        "QuestionId": 175,
        "QuestionDesc": "Which method is commonly used to estimate coefficients in Polynomial Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Random Forest"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "OLS is used to minimize the sum of squared errors between actual and predicted values."
    },
    {
        "QuestionId": 176,
        "QuestionDesc": "Why is Polynomial Regression prone to overfitting?",
        "QuestionOptions": [
            "Because it has too few parameters",
            "Because it assumes a linear relationship",
            "Because high-degree polynomials can fit noise instead of patterns",
            "Because it ignores errors"
        ],
        "QuestionAnswer": "Because high-degree polynomials can fit noise instead of patterns",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Using high-degree polynomials can lead to overfitting, where the model captures noise instead of actual trends."
    },
    {
        "QuestionId": 177,
        "QuestionDesc": "What is the primary difference between Linear Regression and Polynomial Regression?",
        "QuestionOptions": [
            "Polynomial Regression includes polynomial terms to model non-linear relationships.",
            "Linear Regression is always more accurate.",
            "Polynomial Regression cannot be used for predictions.",
            "Polynomial Regression is only used for categorical data."
        ],
        "QuestionAnswer": "Polynomial Regression includes polynomial terms to model non-linear relationships.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression extends Linear Regression by including polynomial terms to capture non-linear patterns."
    },
    {
        "QuestionId": 178,
        "QuestionDesc": "Which of the following is an application of Polynomial Regression?",
        "QuestionOptions": [
            "Predicting continuous numerical values with non-linear trends",
            "Classifying emails as spam or not spam",
            "Grouping customers into segments",
            "Identifying faces in an image"
        ],
        "QuestionAnswer": "Predicting continuous numerical values with non-linear trends",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used in scenarios where the relationship between variables is non-linear."
    },
    {
        "QuestionId": 179,
        "QuestionDesc": "Which metric is used to evaluate the performance of a Polynomial Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Silhouette Score",
            "F1 Score",
            "Confusion Matrix"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 180,
        "QuestionDesc": "How can overfitting in Polynomial Regression be prevented?",
        "QuestionOptions": [
            "By increasing the polynomial degree",
            "By reducing data points",
            "By using regularization techniques like Ridge or Lasso",
            "By removing the intercept"
        ],
        "QuestionAnswer": "By using regularization techniques like Ridge or Lasso",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Regularization techniques like Ridge and Lasso help prevent overfitting by penalizing large coefficients."
    },
    {
        "QuestionId": 181,
        "QuestionDesc": "What is the primary objective of Polynomial Regression?",
        "QuestionOptions": [
            "To model linear relationships",
            "To model non-linear relationships",
            "To reduce dimensionality",
            "To classify data"
        ],
        "QuestionAnswer": "To model non-linear relationships",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used when the relationship between independent and dependent variables is non-linear."
    },
    {
        "QuestionId": 182,
        "QuestionDesc": "Which of the following terms is unique to Polynomial Regression?",
        "QuestionOptions": [
            "Intercept (\u03b20)",
            "Slope (\u03b21)",
            "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
            "Error Term (\u03b5)"
        ],
        "QuestionAnswer": "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression includes polynomial terms like x\u00b2 and x\u00b3 to capture non-linear patterns."
    },
    {
        "QuestionId": 183,
        "QuestionDesc": "What is the equation for Quadratic Regression?",
        "QuestionOptions": [
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
            "y = \u03b20 + \u03b21x",
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2 + \u03b23x\u00b3",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Quadratic Regression is a second-degree Polynomial Regression model with an x\u00b2 term."
    },
    {
        "QuestionId": 184,
        "QuestionDesc": "Which assumption is specific to Polynomial Regression?",
        "QuestionOptions": [
            "The relationship between variables can be approximated using a polynomial function.",
            "The data must be normally distributed.",
            "There should be no missing values.",
            "All variables must be categorical."
        ],
        "QuestionAnswer": "The relationship between variables can be approximated using a polynomial function.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression assumes that a polynomial function can approximate the relationship between variables."
    },
    {
        "QuestionId": 185,
        "QuestionDesc": "Which method is commonly used to estimate coefficients in Polynomial Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Random Forest"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "OLS is used to minimize the sum of squared errors between actual and predicted values."
    },
    {
        "QuestionId": 186,
        "QuestionDesc": "Why is Polynomial Regression prone to overfitting?",
        "QuestionOptions": [
            "Because it has too few parameters",
            "Because it assumes a linear relationship",
            "Because high-degree polynomials can fit noise instead of patterns",
            "Because it ignores errors"
        ],
        "QuestionAnswer": "Because high-degree polynomials can fit noise instead of patterns",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Using high-degree polynomials can lead to overfitting, where the model captures noise instead of actual trends."
    },
    {
        "QuestionId": 187,
        "QuestionDesc": "What is the primary difference between Linear Regression and Polynomial Regression?",
        "QuestionOptions": [
            "Polynomial Regression includes polynomial terms to model non-linear relationships.",
            "Linear Regression is always more accurate.",
            "Polynomial Regression cannot be used for predictions.",
            "Polynomial Regression is only used for categorical data."
        ],
        "QuestionAnswer": "Polynomial Regression includes polynomial terms to model non-linear relationships.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression extends Linear Regression by including polynomial terms to capture non-linear patterns."
    },
    {
        "QuestionId": 188,
        "QuestionDesc": "Which of the following is an application of Polynomial Regression?",
        "QuestionOptions": [
            "Predicting continuous numerical values with non-linear trends",
            "Classifying emails as spam or not spam",
            "Grouping customers into segments",
            "Identifying faces in an image"
        ],
        "QuestionAnswer": "Predicting continuous numerical values with non-linear trends",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used in scenarios where the relationship between variables is non-linear."
    },
    {
        "QuestionId": 189,
        "QuestionDesc": "Which metric is used to evaluate the performance of a Polynomial Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Silhouette Score",
            "F1 Score",
            "Confusion Matrix"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 190,
        "QuestionDesc": "How can overfitting in Polynomial Regression be prevented?",
        "QuestionOptions": [
            "By increasing the polynomial degree",
            "By reducing data points",
            "By using regularization techniques like Ridge or Lasso",
            "By removing the intercept"
        ],
        "QuestionAnswer": "By using regularization techniques like Ridge or Lasso",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Regularization techniques like Ridge and Lasso help prevent overfitting by penalizing large coefficients."
    },
    {
        "QuestionId": 191,
        "QuestionDesc": "What is the primary objective of Polynomial Regression?",
        "QuestionOptions": [
            "To model linear relationships",
            "To model non-linear relationships",
            "To reduce dimensionality",
            "To classify data"
        ],
        "QuestionAnswer": "To model non-linear relationships",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used when the relationship between independent and dependent variables is non-linear."
    },
    {
        "QuestionId": 192,
        "QuestionDesc": "Which of the following terms is unique to Polynomial Regression?",
        "QuestionOptions": [
            "Intercept (\u03b20)",
            "Slope (\u03b21)",
            "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
            "Error Term (\u03b5)"
        ],
        "QuestionAnswer": "Polynomial Terms (x\u00b2, x\u00b3, etc.)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression includes polynomial terms like x\u00b2 and x\u00b3 to capture non-linear patterns."
    },
    {
        "QuestionId": 193,
        "QuestionDesc": "What is the equation for Quadratic Regression?",
        "QuestionOptions": [
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
            "y = \u03b20 + \u03b21x",
            "y = \u03b20 + \u03b21x + \u03b22x\u00b2 + \u03b23x\u00b3",
            "y = log(x)"
        ],
        "QuestionAnswer": "y = \u03b20 + \u03b21x + \u03b22x\u00b2",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Quadratic Regression is a second-degree Polynomial Regression model with an x\u00b2 term."
    },
    {
        "QuestionId": 194,
        "QuestionDesc": "Which assumption is specific to Polynomial Regression?",
        "QuestionOptions": [
            "The relationship between variables can be approximated using a polynomial function.",
            "The data must be normally distributed.",
            "There should be no missing values.",
            "All variables must be categorical."
        ],
        "QuestionAnswer": "The relationship between variables can be approximated using a polynomial function.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression assumes that a polynomial function can approximate the relationship between variables."
    },
    {
        "QuestionId": 195,
        "QuestionDesc": "Which method is commonly used to estimate coefficients in Polynomial Regression?",
        "QuestionOptions": [
            "Gradient Descent",
            "Maximum Likelihood",
            "Ordinary Least Squares (OLS)",
            "Random Forest"
        ],
        "QuestionAnswer": "Ordinary Least Squares (OLS)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "OLS is used to minimize the sum of squared errors between actual and predicted values."
    },
    {
        "QuestionId": 196,
        "QuestionDesc": "Why is Polynomial Regression prone to overfitting?",
        "QuestionOptions": [
            "Because it has too few parameters",
            "Because it assumes a linear relationship",
            "Because high-degree polynomials can fit noise instead of patterns",
            "Because it ignores errors"
        ],
        "QuestionAnswer": "Because high-degree polynomials can fit noise instead of patterns",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Using high-degree polynomials can lead to overfitting, where the model captures noise instead of actual trends."
    },
    {
        "QuestionId": 197,
        "QuestionDesc": "What is the primary difference between Linear Regression and Polynomial Regression?",
        "QuestionOptions": [
            "Polynomial Regression includes polynomial terms to model non-linear relationships.",
            "Linear Regression is always more accurate.",
            "Polynomial Regression cannot be used for predictions.",
            "Polynomial Regression is only used for categorical data."
        ],
        "QuestionAnswer": "Polynomial Regression includes polynomial terms to model non-linear relationships.",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression extends Linear Regression by including polynomial terms to capture non-linear patterns."
    },
    {
        "QuestionId": 198,
        "QuestionDesc": "Which of the following is an application of Polynomial Regression?",
        "QuestionOptions": [
            "Predicting continuous numerical values with non-linear trends",
            "Classifying emails as spam or not spam",
            "Grouping customers into segments",
            "Identifying faces in an image"
        ],
        "QuestionAnswer": "Predicting continuous numerical values with non-linear trends",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Polynomial Regression is used in scenarios where the relationship between variables is non-linear."
    },
    {
        "QuestionId": 199,
        "QuestionDesc": "Which metric is used to evaluate the performance of a Polynomial Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Silhouette Score",
            "F1 Score",
            "Confusion Matrix"
        ],
        "QuestionAnswer": "Mean Squared Error (MSE)",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "MSE calculates the average squared difference between actual and predicted values."
    },
    {
        "QuestionId": 200,
        "QuestionDesc": "How can overfitting in Polynomial Regression be prevented?",
        "QuestionOptions": [
            "By increasing the polynomial degree",
            "By reducing data points",
            "By using regularization techniques like Ridge or Lasso",
            "By removing the intercept"
        ],
        "QuestionAnswer": "By using regularization techniques like Ridge or Lasso",
        "QuestionTopic": "Polynomial Regression",
        "QuestionReason": "Regularization techniques like Ridge and Lasso help prevent overfitting by penalizing large coefficients."
    },
    {
        "QuestionId": 201,
        "QuestionDesc": "What is the primary objective of Logistic Regression?",
        "QuestionOptions": [
            "To model binary classification problems",
            "To model linear relationships",
            "To reduce dimensionality",
            "To cluster data"
        ],
        "QuestionAnswer": "To model binary classification problems",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression is used for binary classification tasks where the outcome variable has two possible values."
    },
    {
        "QuestionId": 202,
        "QuestionDesc": "Which function is used in Logistic Regression to map predictions between 0 and 1?",
        "QuestionOptions": [
            "Sigmoid Function",
            "ReLU Function",
            "Linear Function",
            "Exponential Function"
        ],
        "QuestionAnswer": "Sigmoid Function",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The sigmoid function maps the log-odds to a probability between 0 and 1, making it suitable for classification."
    },
    {
        "QuestionId": 203,
        "QuestionDesc": "What is the log-odds (logit) in Logistic Regression?",
        "QuestionOptions": [
            "The natural logarithm of the odds of the target variable being 1",
            "A probability value between 0 and 1",
            "The sum of squared errors",
            "A measure of variance"
        ],
        "QuestionAnswer": "The natural logarithm of the odds of the target variable being 1",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The log-odds is used to transform the linear combination of inputs into a form suitable for classification."
    },
    {
        "QuestionId": 204,
        "QuestionDesc": "Which type of Logistic Regression is used for multi-class classification?",
        "QuestionOptions": [
            "Binary Logistic Regression",
            "Multinomial Logistic Regression",
            "Ordinal Logistic Regression",
            "Polynomial Regression"
        ],
        "QuestionAnswer": "Multinomial Logistic Regression",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Multinomial Logistic Regression is used when there are more than two outcome categories in the dependent variable."
    },
    {
        "QuestionId": 205,
        "QuestionDesc": "Which assumption is NOT required for Logistic Regression?",
        "QuestionOptions": [
            "The dependent variable must be categorical",
            "The relationship between independent variables and log-odds is linear",
            "Independent variables must follow a normal distribution",
            "Observations should be independent"
        ],
        "QuestionAnswer": "Independent variables must follow a normal distribution",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Unlike linear regression, logistic regression does not assume that independent variables follow a normal distribution."
    },
    {
        "QuestionId": 206,
        "QuestionDesc": "Which loss function is typically used to train a Logistic Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Cross-Entropy Loss",
            "Hinge Loss",
            "Huber Loss"
        ],
        "QuestionAnswer": "Cross-Entropy Loss",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Cross-entropy loss measures the difference between actual and predicted probabilities, making it suitable for classification tasks."
    },
    {
        "QuestionId": 207,
        "QuestionDesc": "What does a threshold of 0.5 in Logistic Regression indicate?",
        "QuestionOptions": [
            "Predictions greater than 0.5 are classified as 1, otherwise 0",
            "Predictions greater than 0.5 are classified as 0, otherwise 1",
            "It controls the learning rate",
            "It determines the number of features"
        ],
        "QuestionAnswer": "Predictions greater than 0.5 are classified as 1, otherwise 0",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "A threshold of 0.5 is commonly used to classify outcomes in binary logistic regression."
    },
    {
        "QuestionId": 208,
        "QuestionDesc": "Which evaluation metric is commonly used for Logistic Regression models?",
        "QuestionOptions": [
            "R-squared",
            "Mean Absolute Error",
            "Accuracy, Precision, Recall, and ROC-AUC",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Accuracy, Precision, Recall, and ROC-AUC",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Metrics like accuracy, precision, recall, and ROC-AUC are used to evaluate the classification performance of logistic regression."
    },
    {
        "QuestionId": 209,
        "QuestionDesc": "What is one advantage of Logistic Regression?",
        "QuestionOptions": [
            "It provides probabilistic predictions",
            "It can only handle linear relationships",
            "It requires a large dataset to work",
            "It always outperforms deep learning models"
        ],
        "QuestionAnswer": "It provides probabilistic predictions",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression provides probabilities for class predictions, making it useful for decision-making."
    },
    {
        "QuestionId": 210,
        "QuestionDesc": "How can class imbalance in Logistic Regression be handled?",
        "QuestionOptions": [
            "Using techniques like oversampling, undersampling, or class weighting",
            "Removing some independent variables",
            "Using only numerical data",
            "Ignoring the problem"
        ],
        "QuestionAnswer": "Using techniques like oversampling, undersampling, or class weighting",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Class imbalance can be addressed using oversampling, undersampling, or adjusting class weights in the model."
    },
    {
        "QuestionId": 211,
        "QuestionDesc": "What is the primary objective of Logistic Regression?",
        "QuestionOptions": [
            "To model binary classification problems",
            "To model linear relationships",
            "To reduce dimensionality",
            "To cluster data"
        ],
        "QuestionAnswer": "To model binary classification problems",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression is used for binary classification tasks where the outcome variable has two possible values."
    },
    {
        "QuestionId": 212,
        "QuestionDesc": "Which function is used in Logistic Regression to map predictions between 0 and 1?",
        "QuestionOptions": [
            "Sigmoid Function",
            "ReLU Function",
            "Linear Function",
            "Exponential Function"
        ],
        "QuestionAnswer": "Sigmoid Function",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The sigmoid function maps the log-odds to a probability between 0 and 1, making it suitable for classification."
    },
    {
        "QuestionId": 213,
        "QuestionDesc": "What is the log-odds (logit) in Logistic Regression?",
        "QuestionOptions": [
            "The natural logarithm of the odds of the target variable being 1",
            "A probability value between 0 and 1",
            "The sum of squared errors",
            "A measure of variance"
        ],
        "QuestionAnswer": "The natural logarithm of the odds of the target variable being 1",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The log-odds is used to transform the linear combination of inputs into a form suitable for classification."
    },
    {
        "QuestionId": 214,
        "QuestionDesc": "Which type of Logistic Regression is used for multi-class classification?",
        "QuestionOptions": [
            "Binary Logistic Regression",
            "Multinomial Logistic Regression",
            "Ordinal Logistic Regression",
            "Polynomial Regression"
        ],
        "QuestionAnswer": "Multinomial Logistic Regression",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Multinomial Logistic Regression is used when there are more than two outcome categories in the dependent variable."
    },
    {
        "QuestionId": 215,
        "QuestionDesc": "Which assumption is NOT required for Logistic Regression?",
        "QuestionOptions": [
            "The dependent variable must be categorical",
            "The relationship between independent variables and log-odds is linear",
            "Independent variables must follow a normal distribution",
            "Observations should be independent"
        ],
        "QuestionAnswer": "Independent variables must follow a normal distribution",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Unlike linear regression, logistic regression does not assume that independent variables follow a normal distribution."
    },
    {
        "QuestionId": 216,
        "QuestionDesc": "Which loss function is typically used to train a Logistic Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Cross-Entropy Loss",
            "Hinge Loss",
            "Huber Loss"
        ],
        "QuestionAnswer": "Cross-Entropy Loss",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Cross-entropy loss measures the difference between actual and predicted probabilities, making it suitable for classification tasks."
    },
    {
        "QuestionId": 217,
        "QuestionDesc": "What does a threshold of 0.5 in Logistic Regression indicate?",
        "QuestionOptions": [
            "Predictions greater than 0.5 are classified as 1, otherwise 0",
            "Predictions greater than 0.5 are classified as 0, otherwise 1",
            "It controls the learning rate",
            "It determines the number of features"
        ],
        "QuestionAnswer": "Predictions greater than 0.5 are classified as 1, otherwise 0",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "A threshold of 0.5 is commonly used to classify outcomes in binary logistic regression."
    },
    {
        "QuestionId": 218,
        "QuestionDesc": "Which evaluation metric is commonly used for Logistic Regression models?",
        "QuestionOptions": [
            "R-squared",
            "Mean Absolute Error",
            "Accuracy, Precision, Recall, and ROC-AUC",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Accuracy, Precision, Recall, and ROC-AUC",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Metrics like accuracy, precision, recall, and ROC-AUC are used to evaluate the classification performance of logistic regression."
    },
    {
        "QuestionId": 219,
        "QuestionDesc": "What is one advantage of Logistic Regression?",
        "QuestionOptions": [
            "It provides probabilistic predictions",
            "It can only handle linear relationships",
            "It requires a large dataset to work",
            "It always outperforms deep learning models"
        ],
        "QuestionAnswer": "It provides probabilistic predictions",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression provides probabilities for class predictions, making it useful for decision-making."
    },
    {
        "QuestionId": 220,
        "QuestionDesc": "How can class imbalance in Logistic Regression be handled?",
        "QuestionOptions": [
            "Using techniques like oversampling, undersampling, or class weighting",
            "Removing some independent variables",
            "Using only numerical data",
            "Ignoring the problem"
        ],
        "QuestionAnswer": "Using techniques like oversampling, undersampling, or class weighting",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Class imbalance can be addressed using oversampling, undersampling, or adjusting class weights in the model."
    },
    {
        "QuestionId": 221,
        "QuestionDesc": "What is the primary objective of Logistic Regression?",
        "QuestionOptions": [
            "To model binary classification problems",
            "To model linear relationships",
            "To reduce dimensionality",
            "To cluster data"
        ],
        "QuestionAnswer": "To model binary classification problems",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression is used for binary classification tasks where the outcome variable has two possible values."
    },
    {
        "QuestionId": 222,
        "QuestionDesc": "Which function is used in Logistic Regression to map predictions between 0 and 1?",
        "QuestionOptions": [
            "Sigmoid Function",
            "ReLU Function",
            "Linear Function",
            "Exponential Function"
        ],
        "QuestionAnswer": "Sigmoid Function",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The sigmoid function maps the log-odds to a probability between 0 and 1, making it suitable for classification."
    },
    {
        "QuestionId": 223,
        "QuestionDesc": "What is the log-odds (logit) in Logistic Regression?",
        "QuestionOptions": [
            "The natural logarithm of the odds of the target variable being 1",
            "A probability value between 0 and 1",
            "The sum of squared errors",
            "A measure of variance"
        ],
        "QuestionAnswer": "The natural logarithm of the odds of the target variable being 1",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The log-odds is used to transform the linear combination of inputs into a form suitable for classification."
    },
    {
        "QuestionId": 224,
        "QuestionDesc": "Which type of Logistic Regression is used for multi-class classification?",
        "QuestionOptions": [
            "Binary Logistic Regression",
            "Multinomial Logistic Regression",
            "Ordinal Logistic Regression",
            "Polynomial Regression"
        ],
        "QuestionAnswer": "Multinomial Logistic Regression",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Multinomial Logistic Regression is used when there are more than two outcome categories in the dependent variable."
    },
    {
        "QuestionId": 225,
        "QuestionDesc": "Which assumption is NOT required for Logistic Regression?",
        "QuestionOptions": [
            "The dependent variable must be categorical",
            "The relationship between independent variables and log-odds is linear",
            "Independent variables must follow a normal distribution",
            "Observations should be independent"
        ],
        "QuestionAnswer": "Independent variables must follow a normal distribution",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Unlike linear regression, logistic regression does not assume that independent variables follow a normal distribution."
    },
    {
        "QuestionId": 226,
        "QuestionDesc": "Which loss function is typically used to train a Logistic Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Cross-Entropy Loss",
            "Hinge Loss",
            "Huber Loss"
        ],
        "QuestionAnswer": "Cross-Entropy Loss",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Cross-entropy loss measures the difference between actual and predicted probabilities, making it suitable for classification tasks."
    },
    {
        "QuestionId": 227,
        "QuestionDesc": "What does a threshold of 0.5 in Logistic Regression indicate?",
        "QuestionOptions": [
            "Predictions greater than 0.5 are classified as 1, otherwise 0",
            "Predictions greater than 0.5 are classified as 0, otherwise 1",
            "It controls the learning rate",
            "It determines the number of features"
        ],
        "QuestionAnswer": "Predictions greater than 0.5 are classified as 1, otherwise 0",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "A threshold of 0.5 is commonly used to classify outcomes in binary logistic regression."
    },
    {
        "QuestionId": 228,
        "QuestionDesc": "Which evaluation metric is commonly used for Logistic Regression models?",
        "QuestionOptions": [
            "R-squared",
            "Mean Absolute Error",
            "Accuracy, Precision, Recall, and ROC-AUC",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Accuracy, Precision, Recall, and ROC-AUC",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Metrics like accuracy, precision, recall, and ROC-AUC are used to evaluate the classification performance of logistic regression."
    },
    {
        "QuestionId": 229,
        "QuestionDesc": "What is one advantage of Logistic Regression?",
        "QuestionOptions": [
            "It provides probabilistic predictions",
            "It can only handle linear relationships",
            "It requires a large dataset to work",
            "It always outperforms deep learning models"
        ],
        "QuestionAnswer": "It provides probabilistic predictions",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression provides probabilities for class predictions, making it useful for decision-making."
    },
    {
        "QuestionId": 230,
        "QuestionDesc": "How can class imbalance in Logistic Regression be handled?",
        "QuestionOptions": [
            "Using techniques like oversampling, undersampling, or class weighting",
            "Removing some independent variables",
            "Using only numerical data",
            "Ignoring the problem"
        ],
        "QuestionAnswer": "Using techniques like oversampling, undersampling, or class weighting",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Class imbalance can be addressed using oversampling, undersampling, or adjusting class weights in the model."
    },
    {
        "QuestionId": 231,
        "QuestionDesc": "What is the primary objective of Logistic Regression?",
        "QuestionOptions": [
            "To model binary classification problems",
            "To model linear relationships",
            "To reduce dimensionality",
            "To cluster data"
        ],
        "QuestionAnswer": "To model binary classification problems",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression is used for binary classification tasks where the outcome variable has two possible values."
    },
    {
        "QuestionId": 232,
        "QuestionDesc": "Which function is used in Logistic Regression to map predictions between 0 and 1?",
        "QuestionOptions": [
            "Sigmoid Function",
            "ReLU Function",
            "Linear Function",
            "Exponential Function"
        ],
        "QuestionAnswer": "Sigmoid Function",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The sigmoid function maps the log-odds to a probability between 0 and 1, making it suitable for classification."
    },
    {
        "QuestionId": 233,
        "QuestionDesc": "What is the log-odds (logit) in Logistic Regression?",
        "QuestionOptions": [
            "The natural logarithm of the odds of the target variable being 1",
            "A probability value between 0 and 1",
            "The sum of squared errors",
            "A measure of variance"
        ],
        "QuestionAnswer": "The natural logarithm of the odds of the target variable being 1",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The log-odds is used to transform the linear combination of inputs into a form suitable for classification."
    },
    {
        "QuestionId": 234,
        "QuestionDesc": "Which type of Logistic Regression is used for multi-class classification?",
        "QuestionOptions": [
            "Binary Logistic Regression",
            "Multinomial Logistic Regression",
            "Ordinal Logistic Regression",
            "Polynomial Regression"
        ],
        "QuestionAnswer": "Multinomial Logistic Regression",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Multinomial Logistic Regression is used when there are more than two outcome categories in the dependent variable."
    },
    {
        "QuestionId": 235,
        "QuestionDesc": "Which assumption is NOT required for Logistic Regression?",
        "QuestionOptions": [
            "The dependent variable must be categorical",
            "The relationship between independent variables and log-odds is linear",
            "Independent variables must follow a normal distribution",
            "Observations should be independent"
        ],
        "QuestionAnswer": "Independent variables must follow a normal distribution",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Unlike linear regression, logistic regression does not assume that independent variables follow a normal distribution."
    },
    {
        "QuestionId": 236,
        "QuestionDesc": "Which loss function is typically used to train a Logistic Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Cross-Entropy Loss",
            "Hinge Loss",
            "Huber Loss"
        ],
        "QuestionAnswer": "Cross-Entropy Loss",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Cross-entropy loss measures the difference between actual and predicted probabilities, making it suitable for classification tasks."
    },
    {
        "QuestionId": 237,
        "QuestionDesc": "What does a threshold of 0.5 in Logistic Regression indicate?",
        "QuestionOptions": [
            "Predictions greater than 0.5 are classified as 1, otherwise 0",
            "Predictions greater than 0.5 are classified as 0, otherwise 1",
            "It controls the learning rate",
            "It determines the number of features"
        ],
        "QuestionAnswer": "Predictions greater than 0.5 are classified as 1, otherwise 0",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "A threshold of 0.5 is commonly used to classify outcomes in binary logistic regression."
    },
    {
        "QuestionId": 238,
        "QuestionDesc": "Which evaluation metric is commonly used for Logistic Regression models?",
        "QuestionOptions": [
            "R-squared",
            "Mean Absolute Error",
            "Accuracy, Precision, Recall, and ROC-AUC",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Accuracy, Precision, Recall, and ROC-AUC",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Metrics like accuracy, precision, recall, and ROC-AUC are used to evaluate the classification performance of logistic regression."
    },
    {
        "QuestionId": 239,
        "QuestionDesc": "What is one advantage of Logistic Regression?",
        "QuestionOptions": [
            "It provides probabilistic predictions",
            "It can only handle linear relationships",
            "It requires a large dataset to work",
            "It always outperforms deep learning models"
        ],
        "QuestionAnswer": "It provides probabilistic predictions",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression provides probabilities for class predictions, making it useful for decision-making."
    },
    {
        "QuestionId": 240,
        "QuestionDesc": "How can class imbalance in Logistic Regression be handled?",
        "QuestionOptions": [
            "Using techniques like oversampling, undersampling, or class weighting",
            "Removing some independent variables",
            "Using only numerical data",
            "Ignoring the problem"
        ],
        "QuestionAnswer": "Using techniques like oversampling, undersampling, or class weighting",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Class imbalance can be addressed using oversampling, undersampling, or adjusting class weights in the model."
    },
    {
        "QuestionId": 241,
        "QuestionDesc": "What is the primary objective of Logistic Regression?",
        "QuestionOptions": [
            "To model binary classification problems",
            "To model linear relationships",
            "To reduce dimensionality",
            "To cluster data"
        ],
        "QuestionAnswer": "To model binary classification problems",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression is used for binary classification tasks where the outcome variable has two possible values."
    },
    {
        "QuestionId": 242,
        "QuestionDesc": "Which function is used in Logistic Regression to map predictions between 0 and 1?",
        "QuestionOptions": [
            "Sigmoid Function",
            "ReLU Function",
            "Linear Function",
            "Exponential Function"
        ],
        "QuestionAnswer": "Sigmoid Function",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The sigmoid function maps the log-odds to a probability between 0 and 1, making it suitable for classification."
    },
    {
        "QuestionId": 243,
        "QuestionDesc": "What is the log-odds (logit) in Logistic Regression?",
        "QuestionOptions": [
            "The natural logarithm of the odds of the target variable being 1",
            "A probability value between 0 and 1",
            "The sum of squared errors",
            "A measure of variance"
        ],
        "QuestionAnswer": "The natural logarithm of the odds of the target variable being 1",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The log-odds is used to transform the linear combination of inputs into a form suitable for classification."
    },
    {
        "QuestionId": 244,
        "QuestionDesc": "Which type of Logistic Regression is used for multi-class classification?",
        "QuestionOptions": [
            "Binary Logistic Regression",
            "Multinomial Logistic Regression",
            "Ordinal Logistic Regression",
            "Polynomial Regression"
        ],
        "QuestionAnswer": "Multinomial Logistic Regression",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Multinomial Logistic Regression is used when there are more than two outcome categories in the dependent variable."
    },
    {
        "QuestionId": 245,
        "QuestionDesc": "Which assumption is NOT required for Logistic Regression?",
        "QuestionOptions": [
            "The dependent variable must be categorical",
            "The relationship between independent variables and log-odds is linear",
            "Independent variables must follow a normal distribution",
            "Observations should be independent"
        ],
        "QuestionAnswer": "Independent variables must follow a normal distribution",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Unlike linear regression, logistic regression does not assume that independent variables follow a normal distribution."
    },
    {
        "QuestionId": 246,
        "QuestionDesc": "Which loss function is typically used to train a Logistic Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Cross-Entropy Loss",
            "Hinge Loss",
            "Huber Loss"
        ],
        "QuestionAnswer": "Cross-Entropy Loss",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Cross-entropy loss measures the difference between actual and predicted probabilities, making it suitable for classification tasks."
    },
    {
        "QuestionId": 247,
        "QuestionDesc": "What does a threshold of 0.5 in Logistic Regression indicate?",
        "QuestionOptions": [
            "Predictions greater than 0.5 are classified as 1, otherwise 0",
            "Predictions greater than 0.5 are classified as 0, otherwise 1",
            "It controls the learning rate",
            "It determines the number of features"
        ],
        "QuestionAnswer": "Predictions greater than 0.5 are classified as 1, otherwise 0",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "A threshold of 0.5 is commonly used to classify outcomes in binary logistic regression."
    },
    {
        "QuestionId": 248,
        "QuestionDesc": "Which evaluation metric is commonly used for Logistic Regression models?",
        "QuestionOptions": [
            "R-squared",
            "Mean Absolute Error",
            "Accuracy, Precision, Recall, and ROC-AUC",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Accuracy, Precision, Recall, and ROC-AUC",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Metrics like accuracy, precision, recall, and ROC-AUC are used to evaluate the classification performance of logistic regression."
    },
    {
        "QuestionId": 249,
        "QuestionDesc": "What is one advantage of Logistic Regression?",
        "QuestionOptions": [
            "It provides probabilistic predictions",
            "It can only handle linear relationships",
            "It requires a large dataset to work",
            "It always outperforms deep learning models"
        ],
        "QuestionAnswer": "It provides probabilistic predictions",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression provides probabilities for class predictions, making it useful for decision-making."
    },
    {
        "QuestionId": 250,
        "QuestionDesc": "How can class imbalance in Logistic Regression be handled?",
        "QuestionOptions": [
            "Using techniques like oversampling, undersampling, or class weighting",
            "Removing some independent variables",
            "Using only numerical data",
            "Ignoring the problem"
        ],
        "QuestionAnswer": "Using techniques like oversampling, undersampling, or class weighting",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Class imbalance can be addressed using oversampling, undersampling, or adjusting class weights in the model."
    },
    {
        "QuestionId": 251,
        "QuestionDesc": "What is the primary objective of Logistic Regression?",
        "QuestionOptions": [
            "To model binary classification problems",
            "To model linear relationships",
            "To reduce dimensionality",
            "To cluster data"
        ],
        "QuestionAnswer": "To model binary classification problems",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression is used for binary classification tasks where the outcome variable has two possible values."
    },
    {
        "QuestionId": 252,
        "QuestionDesc": "Which function is used in Logistic Regression to map predictions between 0 and 1?",
        "QuestionOptions": [
            "Sigmoid Function",
            "ReLU Function",
            "Linear Function",
            "Exponential Function"
        ],
        "QuestionAnswer": "Sigmoid Function",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The sigmoid function maps the log-odds to a probability between 0 and 1, making it suitable for classification."
    },
    {
        "QuestionId": 253,
        "QuestionDesc": "What is the log-odds (logit) in Logistic Regression?",
        "QuestionOptions": [
            "The natural logarithm of the odds of the target variable being 1",
            "A probability value between 0 and 1",
            "The sum of squared errors",
            "A measure of variance"
        ],
        "QuestionAnswer": "The natural logarithm of the odds of the target variable being 1",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The log-odds is used to transform the linear combination of inputs into a form suitable for classification."
    },
    {
        "QuestionId": 254,
        "QuestionDesc": "Which type of Logistic Regression is used for multi-class classification?",
        "QuestionOptions": [
            "Binary Logistic Regression",
            "Multinomial Logistic Regression",
            "Ordinal Logistic Regression",
            "Polynomial Regression"
        ],
        "QuestionAnswer": "Multinomial Logistic Regression",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Multinomial Logistic Regression is used when there are more than two outcome categories in the dependent variable."
    },
    {
        "QuestionId": 255,
        "QuestionDesc": "Which assumption is NOT required for Logistic Regression?",
        "QuestionOptions": [
            "The dependent variable must be categorical",
            "The relationship between independent variables and log-odds is linear",
            "Independent variables must follow a normal distribution",
            "Observations should be independent"
        ],
        "QuestionAnswer": "Independent variables must follow a normal distribution",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Unlike linear regression, logistic regression does not assume that independent variables follow a normal distribution."
    },
    {
        "QuestionId": 256,
        "QuestionDesc": "Which loss function is typically used to train a Logistic Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Cross-Entropy Loss",
            "Hinge Loss",
            "Huber Loss"
        ],
        "QuestionAnswer": "Cross-Entropy Loss",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Cross-entropy loss measures the difference between actual and predicted probabilities, making it suitable for classification tasks."
    },
    {
        "QuestionId": 257,
        "QuestionDesc": "What does a threshold of 0.5 in Logistic Regression indicate?",
        "QuestionOptions": [
            "Predictions greater than 0.5 are classified as 1, otherwise 0",
            "Predictions greater than 0.5 are classified as 0, otherwise 1",
            "It controls the learning rate",
            "It determines the number of features"
        ],
        "QuestionAnswer": "Predictions greater than 0.5 are classified as 1, otherwise 0",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "A threshold of 0.5 is commonly used to classify outcomes in binary logistic regression."
    },
    {
        "QuestionId": 258,
        "QuestionDesc": "Which evaluation metric is commonly used for Logistic Regression models?",
        "QuestionOptions": [
            "R-squared",
            "Mean Absolute Error",
            "Accuracy, Precision, Recall, and ROC-AUC",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Accuracy, Precision, Recall, and ROC-AUC",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Metrics like accuracy, precision, recall, and ROC-AUC are used to evaluate the classification performance of logistic regression."
    },
    {
        "QuestionId": 259,
        "QuestionDesc": "What is one advantage of Logistic Regression?",
        "QuestionOptions": [
            "It provides probabilistic predictions",
            "It can only handle linear relationships",
            "It requires a large dataset to work",
            "It always outperforms deep learning models"
        ],
        "QuestionAnswer": "It provides probabilistic predictions",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression provides probabilities for class predictions, making it useful for decision-making."
    },
    {
        "QuestionId": 260,
        "QuestionDesc": "How can class imbalance in Logistic Regression be handled?",
        "QuestionOptions": [
            "Using techniques like oversampling, undersampling, or class weighting",
            "Removing some independent variables",
            "Using only numerical data",
            "Ignoring the problem"
        ],
        "QuestionAnswer": "Using techniques like oversampling, undersampling, or class weighting",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Class imbalance can be addressed using oversampling, undersampling, or adjusting class weights in the model."
    },
    {
        "QuestionId": 261,
        "QuestionDesc": "What is the primary objective of Logistic Regression?",
        "QuestionOptions": [
            "To model binary classification problems",
            "To model linear relationships",
            "To reduce dimensionality",
            "To cluster data"
        ],
        "QuestionAnswer": "To model binary classification problems",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression is used for binary classification tasks where the outcome variable has two possible values."
    },
    {
        "QuestionId": 262,
        "QuestionDesc": "Which function is used in Logistic Regression to map predictions between 0 and 1?",
        "QuestionOptions": [
            "Sigmoid Function",
            "ReLU Function",
            "Linear Function",
            "Exponential Function"
        ],
        "QuestionAnswer": "Sigmoid Function",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The sigmoid function maps the log-odds to a probability between 0 and 1, making it suitable for classification."
    },
    {
        "QuestionId": 263,
        "QuestionDesc": "What is the log-odds (logit) in Logistic Regression?",
        "QuestionOptions": [
            "The natural logarithm of the odds of the target variable being 1",
            "A probability value between 0 and 1",
            "The sum of squared errors",
            "A measure of variance"
        ],
        "QuestionAnswer": "The natural logarithm of the odds of the target variable being 1",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The log-odds is used to transform the linear combination of inputs into a form suitable for classification."
    },
    {
        "QuestionId": 264,
        "QuestionDesc": "Which type of Logistic Regression is used for multi-class classification?",
        "QuestionOptions": [
            "Binary Logistic Regression",
            "Multinomial Logistic Regression",
            "Ordinal Logistic Regression",
            "Polynomial Regression"
        ],
        "QuestionAnswer": "Multinomial Logistic Regression",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Multinomial Logistic Regression is used when there are more than two outcome categories in the dependent variable."
    },
    {
        "QuestionId": 265,
        "QuestionDesc": "Which assumption is NOT required for Logistic Regression?",
        "QuestionOptions": [
            "The dependent variable must be categorical",
            "The relationship between independent variables and log-odds is linear",
            "Independent variables must follow a normal distribution",
            "Observations should be independent"
        ],
        "QuestionAnswer": "Independent variables must follow a normal distribution",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Unlike linear regression, logistic regression does not assume that independent variables follow a normal distribution."
    },
    {
        "QuestionId": 266,
        "QuestionDesc": "Which loss function is typically used to train a Logistic Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Cross-Entropy Loss",
            "Hinge Loss",
            "Huber Loss"
        ],
        "QuestionAnswer": "Cross-Entropy Loss",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Cross-entropy loss measures the difference between actual and predicted probabilities, making it suitable for classification tasks."
    },
    {
        "QuestionId": 267,
        "QuestionDesc": "What does a threshold of 0.5 in Logistic Regression indicate?",
        "QuestionOptions": [
            "Predictions greater than 0.5 are classified as 1, otherwise 0",
            "Predictions greater than 0.5 are classified as 0, otherwise 1",
            "It controls the learning rate",
            "It determines the number of features"
        ],
        "QuestionAnswer": "Predictions greater than 0.5 are classified as 1, otherwise 0",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "A threshold of 0.5 is commonly used to classify outcomes in binary logistic regression."
    },
    {
        "QuestionId": 268,
        "QuestionDesc": "Which evaluation metric is commonly used for Logistic Regression models?",
        "QuestionOptions": [
            "R-squared",
            "Mean Absolute Error",
            "Accuracy, Precision, Recall, and ROC-AUC",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Accuracy, Precision, Recall, and ROC-AUC",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Metrics like accuracy, precision, recall, and ROC-AUC are used to evaluate the classification performance of logistic regression."
    },
    {
        "QuestionId": 269,
        "QuestionDesc": "What is one advantage of Logistic Regression?",
        "QuestionOptions": [
            "It provides probabilistic predictions",
            "It can only handle linear relationships",
            "It requires a large dataset to work",
            "It always outperforms deep learning models"
        ],
        "QuestionAnswer": "It provides probabilistic predictions",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression provides probabilities for class predictions, making it useful for decision-making."
    },
    {
        "QuestionId": 270,
        "QuestionDesc": "How can class imbalance in Logistic Regression be handled?",
        "QuestionOptions": [
            "Using techniques like oversampling, undersampling, or class weighting",
            "Removing some independent variables",
            "Using only numerical data",
            "Ignoring the problem"
        ],
        "QuestionAnswer": "Using techniques like oversampling, undersampling, or class weighting",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Class imbalance can be addressed using oversampling, undersampling, or adjusting class weights in the model."
    },
    {
        "QuestionId": 271,
        "QuestionDesc": "What is the primary objective of Logistic Regression?",
        "QuestionOptions": [
            "To model binary classification problems",
            "To model linear relationships",
            "To reduce dimensionality",
            "To cluster data"
        ],
        "QuestionAnswer": "To model binary classification problems",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression is used for binary classification tasks where the outcome variable has two possible values."
    },
    {
        "QuestionId": 272,
        "QuestionDesc": "Which function is used in Logistic Regression to map predictions between 0 and 1?",
        "QuestionOptions": [
            "Sigmoid Function",
            "ReLU Function",
            "Linear Function",
            "Exponential Function"
        ],
        "QuestionAnswer": "Sigmoid Function",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The sigmoid function maps the log-odds to a probability between 0 and 1, making it suitable for classification."
    },
    {
        "QuestionId": 273,
        "QuestionDesc": "What is the log-odds (logit) in Logistic Regression?",
        "QuestionOptions": [
            "The natural logarithm of the odds of the target variable being 1",
            "A probability value between 0 and 1",
            "The sum of squared errors",
            "A measure of variance"
        ],
        "QuestionAnswer": "The natural logarithm of the odds of the target variable being 1",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The log-odds is used to transform the linear combination of inputs into a form suitable for classification."
    },
    {
        "QuestionId": 274,
        "QuestionDesc": "Which type of Logistic Regression is used for multi-class classification?",
        "QuestionOptions": [
            "Binary Logistic Regression",
            "Multinomial Logistic Regression",
            "Ordinal Logistic Regression",
            "Polynomial Regression"
        ],
        "QuestionAnswer": "Multinomial Logistic Regression",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Multinomial Logistic Regression is used when there are more than two outcome categories in the dependent variable."
    },
    {
        "QuestionId": 275,
        "QuestionDesc": "Which assumption is NOT required for Logistic Regression?",
        "QuestionOptions": [
            "The dependent variable must be categorical",
            "The relationship between independent variables and log-odds is linear",
            "Independent variables must follow a normal distribution",
            "Observations should be independent"
        ],
        "QuestionAnswer": "Independent variables must follow a normal distribution",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Unlike linear regression, logistic regression does not assume that independent variables follow a normal distribution."
    },
    {
        "QuestionId": 276,
        "QuestionDesc": "Which loss function is typically used to train a Logistic Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Cross-Entropy Loss",
            "Hinge Loss",
            "Huber Loss"
        ],
        "QuestionAnswer": "Cross-Entropy Loss",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Cross-entropy loss measures the difference between actual and predicted probabilities, making it suitable for classification tasks."
    },
    {
        "QuestionId": 277,
        "QuestionDesc": "What does a threshold of 0.5 in Logistic Regression indicate?",
        "QuestionOptions": [
            "Predictions greater than 0.5 are classified as 1, otherwise 0",
            "Predictions greater than 0.5 are classified as 0, otherwise 1",
            "It controls the learning rate",
            "It determines the number of features"
        ],
        "QuestionAnswer": "Predictions greater than 0.5 are classified as 1, otherwise 0",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "A threshold of 0.5 is commonly used to classify outcomes in binary logistic regression."
    },
    {
        "QuestionId": 278,
        "QuestionDesc": "Which evaluation metric is commonly used for Logistic Regression models?",
        "QuestionOptions": [
            "R-squared",
            "Mean Absolute Error",
            "Accuracy, Precision, Recall, and ROC-AUC",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Accuracy, Precision, Recall, and ROC-AUC",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Metrics like accuracy, precision, recall, and ROC-AUC are used to evaluate the classification performance of logistic regression."
    },
    {
        "QuestionId": 279,
        "QuestionDesc": "What is one advantage of Logistic Regression?",
        "QuestionOptions": [
            "It provides probabilistic predictions",
            "It can only handle linear relationships",
            "It requires a large dataset to work",
            "It always outperforms deep learning models"
        ],
        "QuestionAnswer": "It provides probabilistic predictions",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression provides probabilities for class predictions, making it useful for decision-making."
    },
    {
        "QuestionId": 280,
        "QuestionDesc": "How can class imbalance in Logistic Regression be handled?",
        "QuestionOptions": [
            "Using techniques like oversampling, undersampling, or class weighting",
            "Removing some independent variables",
            "Using only numerical data",
            "Ignoring the problem"
        ],
        "QuestionAnswer": "Using techniques like oversampling, undersampling, or class weighting",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Class imbalance can be addressed using oversampling, undersampling, or adjusting class weights in the model."
    },
    {
        "QuestionId": 281,
        "QuestionDesc": "What is the primary objective of Logistic Regression?",
        "QuestionOptions": [
            "To model binary classification problems",
            "To model linear relationships",
            "To reduce dimensionality",
            "To cluster data"
        ],
        "QuestionAnswer": "To model binary classification problems",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression is used for binary classification tasks where the outcome variable has two possible values."
    },
    {
        "QuestionId": 282,
        "QuestionDesc": "Which function is used in Logistic Regression to map predictions between 0 and 1?",
        "QuestionOptions": [
            "Sigmoid Function",
            "ReLU Function",
            "Linear Function",
            "Exponential Function"
        ],
        "QuestionAnswer": "Sigmoid Function",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The sigmoid function maps the log-odds to a probability between 0 and 1, making it suitable for classification."
    },
    {
        "QuestionId": 283,
        "QuestionDesc": "What is the log-odds (logit) in Logistic Regression?",
        "QuestionOptions": [
            "The natural logarithm of the odds of the target variable being 1",
            "A probability value between 0 and 1",
            "The sum of squared errors",
            "A measure of variance"
        ],
        "QuestionAnswer": "The natural logarithm of the odds of the target variable being 1",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The log-odds is used to transform the linear combination of inputs into a form suitable for classification."
    },
    {
        "QuestionId": 284,
        "QuestionDesc": "Which type of Logistic Regression is used for multi-class classification?",
        "QuestionOptions": [
            "Binary Logistic Regression",
            "Multinomial Logistic Regression",
            "Ordinal Logistic Regression",
            "Polynomial Regression"
        ],
        "QuestionAnswer": "Multinomial Logistic Regression",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Multinomial Logistic Regression is used when there are more than two outcome categories in the dependent variable."
    },
    {
        "QuestionId": 285,
        "QuestionDesc": "Which assumption is NOT required for Logistic Regression?",
        "QuestionOptions": [
            "The dependent variable must be categorical",
            "The relationship between independent variables and log-odds is linear",
            "Independent variables must follow a normal distribution",
            "Observations should be independent"
        ],
        "QuestionAnswer": "Independent variables must follow a normal distribution",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Unlike linear regression, logistic regression does not assume that independent variables follow a normal distribution."
    },
    {
        "QuestionId": 286,
        "QuestionDesc": "Which loss function is typically used to train a Logistic Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Cross-Entropy Loss",
            "Hinge Loss",
            "Huber Loss"
        ],
        "QuestionAnswer": "Cross-Entropy Loss",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Cross-entropy loss measures the difference between actual and predicted probabilities, making it suitable for classification tasks."
    },
    {
        "QuestionId": 287,
        "QuestionDesc": "What does a threshold of 0.5 in Logistic Regression indicate?",
        "QuestionOptions": [
            "Predictions greater than 0.5 are classified as 1, otherwise 0",
            "Predictions greater than 0.5 are classified as 0, otherwise 1",
            "It controls the learning rate",
            "It determines the number of features"
        ],
        "QuestionAnswer": "Predictions greater than 0.5 are classified as 1, otherwise 0",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "A threshold of 0.5 is commonly used to classify outcomes in binary logistic regression."
    },
    {
        "QuestionId": 288,
        "QuestionDesc": "Which evaluation metric is commonly used for Logistic Regression models?",
        "QuestionOptions": [
            "R-squared",
            "Mean Absolute Error",
            "Accuracy, Precision, Recall, and ROC-AUC",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Accuracy, Precision, Recall, and ROC-AUC",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Metrics like accuracy, precision, recall, and ROC-AUC are used to evaluate the classification performance of logistic regression."
    },
    {
        "QuestionId": 289,
        "QuestionDesc": "What is one advantage of Logistic Regression?",
        "QuestionOptions": [
            "It provides probabilistic predictions",
            "It can only handle linear relationships",
            "It requires a large dataset to work",
            "It always outperforms deep learning models"
        ],
        "QuestionAnswer": "It provides probabilistic predictions",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression provides probabilities for class predictions, making it useful for decision-making."
    },
    {
        "QuestionId": 290,
        "QuestionDesc": "How can class imbalance in Logistic Regression be handled?",
        "QuestionOptions": [
            "Using techniques like oversampling, undersampling, or class weighting",
            "Removing some independent variables",
            "Using only numerical data",
            "Ignoring the problem"
        ],
        "QuestionAnswer": "Using techniques like oversampling, undersampling, or class weighting",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Class imbalance can be addressed using oversampling, undersampling, or adjusting class weights in the model."
    },
    {
        "QuestionId": 291,
        "QuestionDesc": "What is the primary objective of Logistic Regression?",
        "QuestionOptions": [
            "To model binary classification problems",
            "To model linear relationships",
            "To reduce dimensionality",
            "To cluster data"
        ],
        "QuestionAnswer": "To model binary classification problems",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression is used for binary classification tasks where the outcome variable has two possible values."
    },
    {
        "QuestionId": 292,
        "QuestionDesc": "Which function is used in Logistic Regression to map predictions between 0 and 1?",
        "QuestionOptions": [
            "Sigmoid Function",
            "ReLU Function",
            "Linear Function",
            "Exponential Function"
        ],
        "QuestionAnswer": "Sigmoid Function",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The sigmoid function maps the log-odds to a probability between 0 and 1, making it suitable for classification."
    },
    {
        "QuestionId": 293,
        "QuestionDesc": "What is the log-odds (logit) in Logistic Regression?",
        "QuestionOptions": [
            "The natural logarithm of the odds of the target variable being 1",
            "A probability value between 0 and 1",
            "The sum of squared errors",
            "A measure of variance"
        ],
        "QuestionAnswer": "The natural logarithm of the odds of the target variable being 1",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "The log-odds is used to transform the linear combination of inputs into a form suitable for classification."
    },
    {
        "QuestionId": 294,
        "QuestionDesc": "Which type of Logistic Regression is used for multi-class classification?",
        "QuestionOptions": [
            "Binary Logistic Regression",
            "Multinomial Logistic Regression",
            "Ordinal Logistic Regression",
            "Polynomial Regression"
        ],
        "QuestionAnswer": "Multinomial Logistic Regression",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Multinomial Logistic Regression is used when there are more than two outcome categories in the dependent variable."
    },
    {
        "QuestionId": 295,
        "QuestionDesc": "Which assumption is NOT required for Logistic Regression?",
        "QuestionOptions": [
            "The dependent variable must be categorical",
            "The relationship between independent variables and log-odds is linear",
            "Independent variables must follow a normal distribution",
            "Observations should be independent"
        ],
        "QuestionAnswer": "Independent variables must follow a normal distribution",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Unlike linear regression, logistic regression does not assume that independent variables follow a normal distribution."
    },
    {
        "QuestionId": 296,
        "QuestionDesc": "Which loss function is typically used to train a Logistic Regression model?",
        "QuestionOptions": [
            "Mean Squared Error (MSE)",
            "Cross-Entropy Loss",
            "Hinge Loss",
            "Huber Loss"
        ],
        "QuestionAnswer": "Cross-Entropy Loss",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Cross-entropy loss measures the difference between actual and predicted probabilities, making it suitable for classification tasks."
    },
    {
        "QuestionId": 297,
        "QuestionDesc": "What does a threshold of 0.5 in Logistic Regression indicate?",
        "QuestionOptions": [
            "Predictions greater than 0.5 are classified as 1, otherwise 0",
            "Predictions greater than 0.5 are classified as 0, otherwise 1",
            "It controls the learning rate",
            "It determines the number of features"
        ],
        "QuestionAnswer": "Predictions greater than 0.5 are classified as 1, otherwise 0",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "A threshold of 0.5 is commonly used to classify outcomes in binary logistic regression."
    },
    {
        "QuestionId": 298,
        "QuestionDesc": "Which evaluation metric is commonly used for Logistic Regression models?",
        "QuestionOptions": [
            "R-squared",
            "Mean Absolute Error",
            "Accuracy, Precision, Recall, and ROC-AUC",
            "Silhouette Score"
        ],
        "QuestionAnswer": "Accuracy, Precision, Recall, and ROC-AUC",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Metrics like accuracy, precision, recall, and ROC-AUC are used to evaluate the classification performance of logistic regression."
    },
    {
        "QuestionId": 299,
        "QuestionDesc": "What is one advantage of Logistic Regression?",
        "QuestionOptions": [
            "It provides probabilistic predictions",
            "It can only handle linear relationships",
            "It requires a large dataset to work",
            "It always outperforms deep learning models"
        ],
        "QuestionAnswer": "It provides probabilistic predictions",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Logistic Regression provides probabilities for class predictions, making it useful for decision-making."
    },
    {
        "QuestionId": 300,
        "QuestionDesc": "How can class imbalance in Logistic Regression be handled?",
        "QuestionOptions": [
            "Using techniques like oversampling, undersampling, or class weighting",
            "Removing some independent variables",
            "Using only numerical data",
            "Ignoring the problem"
        ],
        "QuestionAnswer": "Using techniques like oversampling, undersampling, or class weighting",
        "QuestionTopic": "Logistic Regression",
        "QuestionReason": "Class imbalance can be addressed using oversampling, undersampling, or adjusting class weights in the model."
    },
    {
        "QuestionId": 301,
        "QuestionDesc": "What is the primary purpose of a Decision Tree?",
        "QuestionOptions": [
            "Classification only",
            "Regression only",
            "Both classification and regression",
            "Clustering"
        ],
        "QuestionAnswer": "Both classification and regression",
        "QuestionReason": "Decision Trees are used for both classification and regression tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 302,
        "QuestionDesc": "Which of the following is NOT a key component of a Decision Tree?",
        "QuestionOptions": [
            "Root Node",
            "Hidden Layer",
            "Leaf Node",
            "Branches"
        ],
        "QuestionAnswer": "Hidden Layer",
        "QuestionReason": "Hidden layers are part of neural networks, not decision trees.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 303,
        "QuestionDesc": "What is the role of a splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Determine when to stop splitting",
            "Measure the homogeneity of subsets",
            "Reduce overfitting",
            "Prune the tree"
        ],
        "QuestionAnswer": "Measure the homogeneity of subsets",
        "QuestionReason": "Splitting criteria like Gini Impurity and Information Gain help in selecting the best feature to split on.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 304,
        "QuestionDesc": "Which of the following is a commonly used splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Gini Impurity",
            "Mean Squared Error",
            "Gradient Descent",
            "L1 Regularization"
        ],
        "QuestionAnswer": "Gini Impurity",
        "QuestionReason": "Gini Impurity is a common measure of how pure a node is in classification tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 305,
        "QuestionDesc": "What type of Decision Tree is used for continuous target variables?",
        "QuestionOptions": [
            "Classification Tree",
            "Regression Tree",
            "CART Tree",
            "Random Tree"
        ],
        "QuestionAnswer": "Regression Tree",
        "QuestionReason": "Regression Trees predict continuous values rather than categorical classes.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 306,
        "QuestionDesc": "Which method can be used to prevent overfitting in Decision Trees?",
        "QuestionOptions": [
            "Increasing tree depth",
            "Pruning",
            "Adding more nodes",
            "Using fewer training samples"
        ],
        "QuestionAnswer": "Pruning",
        "QuestionReason": "Pruning helps remove unnecessary branches, reducing the risk of overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 307,
        "QuestionDesc": "What is the stopping condition in a Decision Tree algorithm?",
        "QuestionOptions": [
            "Maximum depth reached",
            "Minimum samples per node",
            "No further improvement in splits",
            "All of the above"
        ],
        "QuestionAnswer": "All of the above",
        "QuestionReason": "Stopping conditions include maximum depth, minimum samples, and lack of improvement in splits.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 308,
        "QuestionDesc": "Which ensemble method improves Decision Tree performance?",
        "QuestionOptions": [
            "Random Forest",
            "K-Means Clustering",
            "Principal Component Analysis",
            "Linear Regression"
        ],
        "QuestionAnswer": "Random Forest",
        "QuestionReason": "Random Forest combines multiple decision trees to improve performance and reduce overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 309,
        "QuestionDesc": "Which disadvantage is associated with Decision Trees?",
        "QuestionOptions": [
            "High bias",
            "Overfitting",
            "Difficult interpretation",
            "High computational cost"
        ],
        "QuestionAnswer": "Overfitting",
        "QuestionReason": "Decision Trees are prone to overfitting, especially when deep.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 310,
        "QuestionDesc": "What is pruning in Decision Trees?",
        "QuestionOptions": [
            "Removing redundant branches",
            "Splitting nodes further",
            "Expanding the tree",
            "Increasing computational complexity"
        ],
        "QuestionAnswer": "Removing redundant branches",
        "QuestionReason": "Pruning removes unnecessary parts of the tree to improve generalization.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 311,
        "QuestionDesc": "What is the primary purpose of a Decision Tree?",
        "QuestionOptions": [
            "Classification only",
            "Regression only",
            "Both classification and regression",
            "Clustering"
        ],
        "QuestionAnswer": "Both classification and regression",
        "QuestionReason": "Decision Trees are used for both classification and regression tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 312,
        "QuestionDesc": "Which of the following is NOT a key component of a Decision Tree?",
        "QuestionOptions": [
            "Root Node",
            "Hidden Layer",
            "Leaf Node",
            "Branches"
        ],
        "QuestionAnswer": "Hidden Layer",
        "QuestionReason": "Hidden layers are part of neural networks, not decision trees.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 313,
        "QuestionDesc": "What is the role of a splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Determine when to stop splitting",
            "Measure the homogeneity of subsets",
            "Reduce overfitting",
            "Prune the tree"
        ],
        "QuestionAnswer": "Measure the homogeneity of subsets",
        "QuestionReason": "Splitting criteria like Gini Impurity and Information Gain help in selecting the best feature to split on.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 314,
        "QuestionDesc": "Which of the following is a commonly used splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Gini Impurity",
            "Mean Squared Error",
            "Gradient Descent",
            "L1 Regularization"
        ],
        "QuestionAnswer": "Gini Impurity",
        "QuestionReason": "Gini Impurity is a common measure of how pure a node is in classification tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 315,
        "QuestionDesc": "What type of Decision Tree is used for continuous target variables?",
        "QuestionOptions": [
            "Classification Tree",
            "Regression Tree",
            "CART Tree",
            "Random Tree"
        ],
        "QuestionAnswer": "Regression Tree",
        "QuestionReason": "Regression Trees predict continuous values rather than categorical classes.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 316,
        "QuestionDesc": "Which method can be used to prevent overfitting in Decision Trees?",
        "QuestionOptions": [
            "Increasing tree depth",
            "Pruning",
            "Adding more nodes",
            "Using fewer training samples"
        ],
        "QuestionAnswer": "Pruning",
        "QuestionReason": "Pruning helps remove unnecessary branches, reducing the risk of overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 317,
        "QuestionDesc": "What is the stopping condition in a Decision Tree algorithm?",
        "QuestionOptions": [
            "Maximum depth reached",
            "Minimum samples per node",
            "No further improvement in splits",
            "All of the above"
        ],
        "QuestionAnswer": "All of the above",
        "QuestionReason": "Stopping conditions include maximum depth, minimum samples, and lack of improvement in splits.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 318,
        "QuestionDesc": "Which ensemble method improves Decision Tree performance?",
        "QuestionOptions": [
            "Random Forest",
            "K-Means Clustering",
            "Principal Component Analysis",
            "Linear Regression"
        ],
        "QuestionAnswer": "Random Forest",
        "QuestionReason": "Random Forest combines multiple decision trees to improve performance and reduce overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 319,
        "QuestionDesc": "Which disadvantage is associated with Decision Trees?",
        "QuestionOptions": [
            "High bias",
            "Overfitting",
            "Difficult interpretation",
            "High computational cost"
        ],
        "QuestionAnswer": "Overfitting",
        "QuestionReason": "Decision Trees are prone to overfitting, especially when deep.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 320,
        "QuestionDesc": "What is pruning in Decision Trees?",
        "QuestionOptions": [
            "Removing redundant branches",
            "Splitting nodes further",
            "Expanding the tree",
            "Increasing computational complexity"
        ],
        "QuestionAnswer": "Removing redundant branches",
        "QuestionReason": "Pruning removes unnecessary parts of the tree to improve generalization.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 321,
        "QuestionDesc": "What is the primary purpose of a Decision Tree?",
        "QuestionOptions": [
            "Classification only",
            "Regression only",
            "Both classification and regression",
            "Clustering"
        ],
        "QuestionAnswer": "Both classification and regression",
        "QuestionReason": "Decision Trees are used for both classification and regression tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 322,
        "QuestionDesc": "Which of the following is NOT a key component of a Decision Tree?",
        "QuestionOptions": [
            "Root Node",
            "Hidden Layer",
            "Leaf Node",
            "Branches"
        ],
        "QuestionAnswer": "Hidden Layer",
        "QuestionReason": "Hidden layers are part of neural networks, not decision trees.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 323,
        "QuestionDesc": "What is the role of a splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Determine when to stop splitting",
            "Measure the homogeneity of subsets",
            "Reduce overfitting",
            "Prune the tree"
        ],
        "QuestionAnswer": "Measure the homogeneity of subsets",
        "QuestionReason": "Splitting criteria like Gini Impurity and Information Gain help in selecting the best feature to split on.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 324,
        "QuestionDesc": "Which of the following is a commonly used splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Gini Impurity",
            "Mean Squared Error",
            "Gradient Descent",
            "L1 Regularization"
        ],
        "QuestionAnswer": "Gini Impurity",
        "QuestionReason": "Gini Impurity is a common measure of how pure a node is in classification tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 325,
        "QuestionDesc": "What type of Decision Tree is used for continuous target variables?",
        "QuestionOptions": [
            "Classification Tree",
            "Regression Tree",
            "CART Tree",
            "Random Tree"
        ],
        "QuestionAnswer": "Regression Tree",
        "QuestionReason": "Regression Trees predict continuous values rather than categorical classes.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 326,
        "QuestionDesc": "Which method can be used to prevent overfitting in Decision Trees?",
        "QuestionOptions": [
            "Increasing tree depth",
            "Pruning",
            "Adding more nodes",
            "Using fewer training samples"
        ],
        "QuestionAnswer": "Pruning",
        "QuestionReason": "Pruning helps remove unnecessary branches, reducing the risk of overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 327,
        "QuestionDesc": "What is the stopping condition in a Decision Tree algorithm?",
        "QuestionOptions": [
            "Maximum depth reached",
            "Minimum samples per node",
            "No further improvement in splits",
            "All of the above"
        ],
        "QuestionAnswer": "All of the above",
        "QuestionReason": "Stopping conditions include maximum depth, minimum samples, and lack of improvement in splits.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 328,
        "QuestionDesc": "Which ensemble method improves Decision Tree performance?",
        "QuestionOptions": [
            "Random Forest",
            "K-Means Clustering",
            "Principal Component Analysis",
            "Linear Regression"
        ],
        "QuestionAnswer": "Random Forest",
        "QuestionReason": "Random Forest combines multiple decision trees to improve performance and reduce overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 329,
        "QuestionDesc": "Which disadvantage is associated with Decision Trees?",
        "QuestionOptions": [
            "High bias",
            "Overfitting",
            "Difficult interpretation",
            "High computational cost"
        ],
        "QuestionAnswer": "Overfitting",
        "QuestionReason": "Decision Trees are prone to overfitting, especially when deep.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 330,
        "QuestionDesc": "What is pruning in Decision Trees?",
        "QuestionOptions": [
            "Removing redundant branches",
            "Splitting nodes further",
            "Expanding the tree",
            "Increasing computational complexity"
        ],
        "QuestionAnswer": "Removing redundant branches",
        "QuestionReason": "Pruning removes unnecessary parts of the tree to improve generalization.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 331,
        "QuestionDesc": "What is the primary purpose of a Decision Tree?",
        "QuestionOptions": [
            "Classification only",
            "Regression only",
            "Both classification and regression",
            "Clustering"
        ],
        "QuestionAnswer": "Both classification and regression",
        "QuestionReason": "Decision Trees are used for both classification and regression tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 332,
        "QuestionDesc": "Which of the following is NOT a key component of a Decision Tree?",
        "QuestionOptions": [
            "Root Node",
            "Hidden Layer",
            "Leaf Node",
            "Branches"
        ],
        "QuestionAnswer": "Hidden Layer",
        "QuestionReason": "Hidden layers are part of neural networks, not decision trees.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 333,
        "QuestionDesc": "What is the role of a splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Determine when to stop splitting",
            "Measure the homogeneity of subsets",
            "Reduce overfitting",
            "Prune the tree"
        ],
        "QuestionAnswer": "Measure the homogeneity of subsets",
        "QuestionReason": "Splitting criteria like Gini Impurity and Information Gain help in selecting the best feature to split on.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 334,
        "QuestionDesc": "Which of the following is a commonly used splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Gini Impurity",
            "Mean Squared Error",
            "Gradient Descent",
            "L1 Regularization"
        ],
        "QuestionAnswer": "Gini Impurity",
        "QuestionReason": "Gini Impurity is a common measure of how pure a node is in classification tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 335,
        "QuestionDesc": "What type of Decision Tree is used for continuous target variables?",
        "QuestionOptions": [
            "Classification Tree",
            "Regression Tree",
            "CART Tree",
            "Random Tree"
        ],
        "QuestionAnswer": "Regression Tree",
        "QuestionReason": "Regression Trees predict continuous values rather than categorical classes.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 336,
        "QuestionDesc": "Which method can be used to prevent overfitting in Decision Trees?",
        "QuestionOptions": [
            "Increasing tree depth",
            "Pruning",
            "Adding more nodes",
            "Using fewer training samples"
        ],
        "QuestionAnswer": "Pruning",
        "QuestionReason": "Pruning helps remove unnecessary branches, reducing the risk of overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 337,
        "QuestionDesc": "What is the stopping condition in a Decision Tree algorithm?",
        "QuestionOptions": [
            "Maximum depth reached",
            "Minimum samples per node",
            "No further improvement in splits",
            "All of the above"
        ],
        "QuestionAnswer": "All of the above",
        "QuestionReason": "Stopping conditions include maximum depth, minimum samples, and lack of improvement in splits.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 338,
        "QuestionDesc": "Which ensemble method improves Decision Tree performance?",
        "QuestionOptions": [
            "Random Forest",
            "K-Means Clustering",
            "Principal Component Analysis",
            "Linear Regression"
        ],
        "QuestionAnswer": "Random Forest",
        "QuestionReason": "Random Forest combines multiple decision trees to improve performance and reduce overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 339,
        "QuestionDesc": "Which disadvantage is associated with Decision Trees?",
        "QuestionOptions": [
            "High bias",
            "Overfitting",
            "Difficult interpretation",
            "High computational cost"
        ],
        "QuestionAnswer": "Overfitting",
        "QuestionReason": "Decision Trees are prone to overfitting, especially when deep.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 340,
        "QuestionDesc": "What is pruning in Decision Trees?",
        "QuestionOptions": [
            "Removing redundant branches",
            "Splitting nodes further",
            "Expanding the tree",
            "Increasing computational complexity"
        ],
        "QuestionAnswer": "Removing redundant branches",
        "QuestionReason": "Pruning removes unnecessary parts of the tree to improve generalization.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 341,
        "QuestionDesc": "What is the primary purpose of a Decision Tree?",
        "QuestionOptions": [
            "Classification only",
            "Regression only",
            "Both classification and regression",
            "Clustering"
        ],
        "QuestionAnswer": "Both classification and regression",
        "QuestionReason": "Decision Trees are used for both classification and regression tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 342,
        "QuestionDesc": "Which of the following is NOT a key component of a Decision Tree?",
        "QuestionOptions": [
            "Root Node",
            "Hidden Layer",
            "Leaf Node",
            "Branches"
        ],
        "QuestionAnswer": "Hidden Layer",
        "QuestionReason": "Hidden layers are part of neural networks, not decision trees.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 343,
        "QuestionDesc": "What is the role of a splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Determine when to stop splitting",
            "Measure the homogeneity of subsets",
            "Reduce overfitting",
            "Prune the tree"
        ],
        "QuestionAnswer": "Measure the homogeneity of subsets",
        "QuestionReason": "Splitting criteria like Gini Impurity and Information Gain help in selecting the best feature to split on.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 344,
        "QuestionDesc": "Which of the following is a commonly used splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Gini Impurity",
            "Mean Squared Error",
            "Gradient Descent",
            "L1 Regularization"
        ],
        "QuestionAnswer": "Gini Impurity",
        "QuestionReason": "Gini Impurity is a common measure of how pure a node is in classification tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 345,
        "QuestionDesc": "What type of Decision Tree is used for continuous target variables?",
        "QuestionOptions": [
            "Classification Tree",
            "Regression Tree",
            "CART Tree",
            "Random Tree"
        ],
        "QuestionAnswer": "Regression Tree",
        "QuestionReason": "Regression Trees predict continuous values rather than categorical classes.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 346,
        "QuestionDesc": "Which method can be used to prevent overfitting in Decision Trees?",
        "QuestionOptions": [
            "Increasing tree depth",
            "Pruning",
            "Adding more nodes",
            "Using fewer training samples"
        ],
        "QuestionAnswer": "Pruning",
        "QuestionReason": "Pruning helps remove unnecessary branches, reducing the risk of overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 347,
        "QuestionDesc": "What is the stopping condition in a Decision Tree algorithm?",
        "QuestionOptions": [
            "Maximum depth reached",
            "Minimum samples per node",
            "No further improvement in splits",
            "All of the above"
        ],
        "QuestionAnswer": "All of the above",
        "QuestionReason": "Stopping conditions include maximum depth, minimum samples, and lack of improvement in splits.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 348,
        "QuestionDesc": "Which ensemble method improves Decision Tree performance?",
        "QuestionOptions": [
            "Random Forest",
            "K-Means Clustering",
            "Principal Component Analysis",
            "Linear Regression"
        ],
        "QuestionAnswer": "Random Forest",
        "QuestionReason": "Random Forest combines multiple decision trees to improve performance and reduce overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 349,
        "QuestionDesc": "Which disadvantage is associated with Decision Trees?",
        "QuestionOptions": [
            "High bias",
            "Overfitting",
            "Difficult interpretation",
            "High computational cost"
        ],
        "QuestionAnswer": "Overfitting",
        "QuestionReason": "Decision Trees are prone to overfitting, especially when deep.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 350,
        "QuestionDesc": "What is pruning in Decision Trees?",
        "QuestionOptions": [
            "Removing redundant branches",
            "Splitting nodes further",
            "Expanding the tree",
            "Increasing computational complexity"
        ],
        "QuestionAnswer": "Removing redundant branches",
        "QuestionReason": "Pruning removes unnecessary parts of the tree to improve generalization.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 351,
        "QuestionDesc": "What is the primary purpose of a Decision Tree?",
        "QuestionOptions": [
            "Classification only",
            "Regression only",
            "Both classification and regression",
            "Clustering"
        ],
        "QuestionAnswer": "Both classification and regression",
        "QuestionReason": "Decision Trees are used for both classification and regression tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 352,
        "QuestionDesc": "Which of the following is NOT a key component of a Decision Tree?",
        "QuestionOptions": [
            "Root Node",
            "Hidden Layer",
            "Leaf Node",
            "Branches"
        ],
        "QuestionAnswer": "Hidden Layer",
        "QuestionReason": "Hidden layers are part of neural networks, not decision trees.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 353,
        "QuestionDesc": "What is the role of a splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Determine when to stop splitting",
            "Measure the homogeneity of subsets",
            "Reduce overfitting",
            "Prune the tree"
        ],
        "QuestionAnswer": "Measure the homogeneity of subsets",
        "QuestionReason": "Splitting criteria like Gini Impurity and Information Gain help in selecting the best feature to split on.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 354,
        "QuestionDesc": "Which of the following is a commonly used splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Gini Impurity",
            "Mean Squared Error",
            "Gradient Descent",
            "L1 Regularization"
        ],
        "QuestionAnswer": "Gini Impurity",
        "QuestionReason": "Gini Impurity is a common measure of how pure a node is in classification tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 355,
        "QuestionDesc": "What type of Decision Tree is used for continuous target variables?",
        "QuestionOptions": [
            "Classification Tree",
            "Regression Tree",
            "CART Tree",
            "Random Tree"
        ],
        "QuestionAnswer": "Regression Tree",
        "QuestionReason": "Regression Trees predict continuous values rather than categorical classes.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 356,
        "QuestionDesc": "Which method can be used to prevent overfitting in Decision Trees?",
        "QuestionOptions": [
            "Increasing tree depth",
            "Pruning",
            "Adding more nodes",
            "Using fewer training samples"
        ],
        "QuestionAnswer": "Pruning",
        "QuestionReason": "Pruning helps remove unnecessary branches, reducing the risk of overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 357,
        "QuestionDesc": "What is the stopping condition in a Decision Tree algorithm?",
        "QuestionOptions": [
            "Maximum depth reached",
            "Minimum samples per node",
            "No further improvement in splits",
            "All of the above"
        ],
        "QuestionAnswer": "All of the above",
        "QuestionReason": "Stopping conditions include maximum depth, minimum samples, and lack of improvement in splits.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 358,
        "QuestionDesc": "Which ensemble method improves Decision Tree performance?",
        "QuestionOptions": [
            "Random Forest",
            "K-Means Clustering",
            "Principal Component Analysis",
            "Linear Regression"
        ],
        "QuestionAnswer": "Random Forest",
        "QuestionReason": "Random Forest combines multiple decision trees to improve performance and reduce overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 359,
        "QuestionDesc": "Which disadvantage is associated with Decision Trees?",
        "QuestionOptions": [
            "High bias",
            "Overfitting",
            "Difficult interpretation",
            "High computational cost"
        ],
        "QuestionAnswer": "Overfitting",
        "QuestionReason": "Decision Trees are prone to overfitting, especially when deep.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 360,
        "QuestionDesc": "What is pruning in Decision Trees?",
        "QuestionOptions": [
            "Removing redundant branches",
            "Splitting nodes further",
            "Expanding the tree",
            "Increasing computational complexity"
        ],
        "QuestionAnswer": "Removing redundant branches",
        "QuestionReason": "Pruning removes unnecessary parts of the tree to improve generalization.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 361,
        "QuestionDesc": "What is the primary purpose of a Decision Tree?",
        "QuestionOptions": [
            "Classification only",
            "Regression only",
            "Both classification and regression",
            "Clustering"
        ],
        "QuestionAnswer": "Both classification and regression",
        "QuestionReason": "Decision Trees are used for both classification and regression tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 362,
        "QuestionDesc": "Which of the following is NOT a key component of a Decision Tree?",
        "QuestionOptions": [
            "Root Node",
            "Hidden Layer",
            "Leaf Node",
            "Branches"
        ],
        "QuestionAnswer": "Hidden Layer",
        "QuestionReason": "Hidden layers are part of neural networks, not decision trees.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 363,
        "QuestionDesc": "What is the role of a splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Determine when to stop splitting",
            "Measure the homogeneity of subsets",
            "Reduce overfitting",
            "Prune the tree"
        ],
        "QuestionAnswer": "Measure the homogeneity of subsets",
        "QuestionReason": "Splitting criteria like Gini Impurity and Information Gain help in selecting the best feature to split on.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 364,
        "QuestionDesc": "Which of the following is a commonly used splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Gini Impurity",
            "Mean Squared Error",
            "Gradient Descent",
            "L1 Regularization"
        ],
        "QuestionAnswer": "Gini Impurity",
        "QuestionReason": "Gini Impurity is a common measure of how pure a node is in classification tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 365,
        "QuestionDesc": "What type of Decision Tree is used for continuous target variables?",
        "QuestionOptions": [
            "Classification Tree",
            "Regression Tree",
            "CART Tree",
            "Random Tree"
        ],
        "QuestionAnswer": "Regression Tree",
        "QuestionReason": "Regression Trees predict continuous values rather than categorical classes.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 366,
        "QuestionDesc": "Which method can be used to prevent overfitting in Decision Trees?",
        "QuestionOptions": [
            "Increasing tree depth",
            "Pruning",
            "Adding more nodes",
            "Using fewer training samples"
        ],
        "QuestionAnswer": "Pruning",
        "QuestionReason": "Pruning helps remove unnecessary branches, reducing the risk of overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 367,
        "QuestionDesc": "What is the stopping condition in a Decision Tree algorithm?",
        "QuestionOptions": [
            "Maximum depth reached",
            "Minimum samples per node",
            "No further improvement in splits",
            "All of the above"
        ],
        "QuestionAnswer": "All of the above",
        "QuestionReason": "Stopping conditions include maximum depth, minimum samples, and lack of improvement in splits.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 368,
        "QuestionDesc": "Which ensemble method improves Decision Tree performance?",
        "QuestionOptions": [
            "Random Forest",
            "K-Means Clustering",
            "Principal Component Analysis",
            "Linear Regression"
        ],
        "QuestionAnswer": "Random Forest",
        "QuestionReason": "Random Forest combines multiple decision trees to improve performance and reduce overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 369,
        "QuestionDesc": "Which disadvantage is associated with Decision Trees?",
        "QuestionOptions": [
            "High bias",
            "Overfitting",
            "Difficult interpretation",
            "High computational cost"
        ],
        "QuestionAnswer": "Overfitting",
        "QuestionReason": "Decision Trees are prone to overfitting, especially when deep.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 370,
        "QuestionDesc": "What is pruning in Decision Trees?",
        "QuestionOptions": [
            "Removing redundant branches",
            "Splitting nodes further",
            "Expanding the tree",
            "Increasing computational complexity"
        ],
        "QuestionAnswer": "Removing redundant branches",
        "QuestionReason": "Pruning removes unnecessary parts of the tree to improve generalization.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 371,
        "QuestionDesc": "What is the primary purpose of a Decision Tree?",
        "QuestionOptions": [
            "Classification only",
            "Regression only",
            "Both classification and regression",
            "Clustering"
        ],
        "QuestionAnswer": "Both classification and regression",
        "QuestionReason": "Decision Trees are used for both classification and regression tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 372,
        "QuestionDesc": "Which of the following is NOT a key component of a Decision Tree?",
        "QuestionOptions": [
            "Root Node",
            "Hidden Layer",
            "Leaf Node",
            "Branches"
        ],
        "QuestionAnswer": "Hidden Layer",
        "QuestionReason": "Hidden layers are part of neural networks, not decision trees.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 373,
        "QuestionDesc": "What is the role of a splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Determine when to stop splitting",
            "Measure the homogeneity of subsets",
            "Reduce overfitting",
            "Prune the tree"
        ],
        "QuestionAnswer": "Measure the homogeneity of subsets",
        "QuestionReason": "Splitting criteria like Gini Impurity and Information Gain help in selecting the best feature to split on.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 374,
        "QuestionDesc": "Which of the following is a commonly used splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Gini Impurity",
            "Mean Squared Error",
            "Gradient Descent",
            "L1 Regularization"
        ],
        "QuestionAnswer": "Gini Impurity",
        "QuestionReason": "Gini Impurity is a common measure of how pure a node is in classification tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 375,
        "QuestionDesc": "What type of Decision Tree is used for continuous target variables?",
        "QuestionOptions": [
            "Classification Tree",
            "Regression Tree",
            "CART Tree",
            "Random Tree"
        ],
        "QuestionAnswer": "Regression Tree",
        "QuestionReason": "Regression Trees predict continuous values rather than categorical classes.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 376,
        "QuestionDesc": "Which method can be used to prevent overfitting in Decision Trees?",
        "QuestionOptions": [
            "Increasing tree depth",
            "Pruning",
            "Adding more nodes",
            "Using fewer training samples"
        ],
        "QuestionAnswer": "Pruning",
        "QuestionReason": "Pruning helps remove unnecessary branches, reducing the risk of overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 377,
        "QuestionDesc": "What is the stopping condition in a Decision Tree algorithm?",
        "QuestionOptions": [
            "Maximum depth reached",
            "Minimum samples per node",
            "No further improvement in splits",
            "All of the above"
        ],
        "QuestionAnswer": "All of the above",
        "QuestionReason": "Stopping conditions include maximum depth, minimum samples, and lack of improvement in splits.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 378,
        "QuestionDesc": "Which ensemble method improves Decision Tree performance?",
        "QuestionOptions": [
            "Random Forest",
            "K-Means Clustering",
            "Principal Component Analysis",
            "Linear Regression"
        ],
        "QuestionAnswer": "Random Forest",
        "QuestionReason": "Random Forest combines multiple decision trees to improve performance and reduce overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 379,
        "QuestionDesc": "Which disadvantage is associated with Decision Trees?",
        "QuestionOptions": [
            "High bias",
            "Overfitting",
            "Difficult interpretation",
            "High computational cost"
        ],
        "QuestionAnswer": "Overfitting",
        "QuestionReason": "Decision Trees are prone to overfitting, especially when deep.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 380,
        "QuestionDesc": "What is pruning in Decision Trees?",
        "QuestionOptions": [
            "Removing redundant branches",
            "Splitting nodes further",
            "Expanding the tree",
            "Increasing computational complexity"
        ],
        "QuestionAnswer": "Removing redundant branches",
        "QuestionReason": "Pruning removes unnecessary parts of the tree to improve generalization.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 381,
        "QuestionDesc": "What is the primary purpose of a Decision Tree?",
        "QuestionOptions": [
            "Classification only",
            "Regression only",
            "Both classification and regression",
            "Clustering"
        ],
        "QuestionAnswer": "Both classification and regression",
        "QuestionReason": "Decision Trees are used for both classification and regression tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 382,
        "QuestionDesc": "Which of the following is NOT a key component of a Decision Tree?",
        "QuestionOptions": [
            "Root Node",
            "Hidden Layer",
            "Leaf Node",
            "Branches"
        ],
        "QuestionAnswer": "Hidden Layer",
        "QuestionReason": "Hidden layers are part of neural networks, not decision trees.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 383,
        "QuestionDesc": "What is the role of a splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Determine when to stop splitting",
            "Measure the homogeneity of subsets",
            "Reduce overfitting",
            "Prune the tree"
        ],
        "QuestionAnswer": "Measure the homogeneity of subsets",
        "QuestionReason": "Splitting criteria like Gini Impurity and Information Gain help in selecting the best feature to split on.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 384,
        "QuestionDesc": "Which of the following is a commonly used splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Gini Impurity",
            "Mean Squared Error",
            "Gradient Descent",
            "L1 Regularization"
        ],
        "QuestionAnswer": "Gini Impurity",
        "QuestionReason": "Gini Impurity is a common measure of how pure a node is in classification tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 385,
        "QuestionDesc": "What type of Decision Tree is used for continuous target variables?",
        "QuestionOptions": [
            "Classification Tree",
            "Regression Tree",
            "CART Tree",
            "Random Tree"
        ],
        "QuestionAnswer": "Regression Tree",
        "QuestionReason": "Regression Trees predict continuous values rather than categorical classes.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 386,
        "QuestionDesc": "Which method can be used to prevent overfitting in Decision Trees?",
        "QuestionOptions": [
            "Increasing tree depth",
            "Pruning",
            "Adding more nodes",
            "Using fewer training samples"
        ],
        "QuestionAnswer": "Pruning",
        "QuestionReason": "Pruning helps remove unnecessary branches, reducing the risk of overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 387,
        "QuestionDesc": "What is the stopping condition in a Decision Tree algorithm?",
        "QuestionOptions": [
            "Maximum depth reached",
            "Minimum samples per node",
            "No further improvement in splits",
            "All of the above"
        ],
        "QuestionAnswer": "All of the above",
        "QuestionReason": "Stopping conditions include maximum depth, minimum samples, and lack of improvement in splits.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 388,
        "QuestionDesc": "Which ensemble method improves Decision Tree performance?",
        "QuestionOptions": [
            "Random Forest",
            "K-Means Clustering",
            "Principal Component Analysis",
            "Linear Regression"
        ],
        "QuestionAnswer": "Random Forest",
        "QuestionReason": "Random Forest combines multiple decision trees to improve performance and reduce overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 389,
        "QuestionDesc": "Which disadvantage is associated with Decision Trees?",
        "QuestionOptions": [
            "High bias",
            "Overfitting",
            "Difficult interpretation",
            "High computational cost"
        ],
        "QuestionAnswer": "Overfitting",
        "QuestionReason": "Decision Trees are prone to overfitting, especially when deep.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 390,
        "QuestionDesc": "What is pruning in Decision Trees?",
        "QuestionOptions": [
            "Removing redundant branches",
            "Splitting nodes further",
            "Expanding the tree",
            "Increasing computational complexity"
        ],
        "QuestionAnswer": "Removing redundant branches",
        "QuestionReason": "Pruning removes unnecessary parts of the tree to improve generalization.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 391,
        "QuestionDesc": "What is the primary purpose of a Decision Tree?",
        "QuestionOptions": [
            "Classification only",
            "Regression only",
            "Both classification and regression",
            "Clustering"
        ],
        "QuestionAnswer": "Both classification and regression",
        "QuestionReason": "Decision Trees are used for both classification and regression tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 392,
        "QuestionDesc": "Which of the following is NOT a key component of a Decision Tree?",
        "QuestionOptions": [
            "Root Node",
            "Hidden Layer",
            "Leaf Node",
            "Branches"
        ],
        "QuestionAnswer": "Hidden Layer",
        "QuestionReason": "Hidden layers are part of neural networks, not decision trees.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 393,
        "QuestionDesc": "What is the role of a splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Determine when to stop splitting",
            "Measure the homogeneity of subsets",
            "Reduce overfitting",
            "Prune the tree"
        ],
        "QuestionAnswer": "Measure the homogeneity of subsets",
        "QuestionReason": "Splitting criteria like Gini Impurity and Information Gain help in selecting the best feature to split on.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 394,
        "QuestionDesc": "Which of the following is a commonly used splitting criterion in Decision Trees?",
        "QuestionOptions": [
            "Gini Impurity",
            "Mean Squared Error",
            "Gradient Descent",
            "L1 Regularization"
        ],
        "QuestionAnswer": "Gini Impurity",
        "QuestionReason": "Gini Impurity is a common measure of how pure a node is in classification tasks.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 395,
        "QuestionDesc": "What type of Decision Tree is used for continuous target variables?",
        "QuestionOptions": [
            "Classification Tree",
            "Regression Tree",
            "CART Tree",
            "Random Tree"
        ],
        "QuestionAnswer": "Regression Tree",
        "QuestionReason": "Regression Trees predict continuous values rather than categorical classes.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 396,
        "QuestionDesc": "Which method can be used to prevent overfitting in Decision Trees?",
        "QuestionOptions": [
            "Increasing tree depth",
            "Pruning",
            "Adding more nodes",
            "Using fewer training samples"
        ],
        "QuestionAnswer": "Pruning",
        "QuestionReason": "Pruning helps remove unnecessary branches, reducing the risk of overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 397,
        "QuestionDesc": "What is the stopping condition in a Decision Tree algorithm?",
        "QuestionOptions": [
            "Maximum depth reached",
            "Minimum samples per node",
            "No further improvement in splits",
            "All of the above"
        ],
        "QuestionAnswer": "All of the above",
        "QuestionReason": "Stopping conditions include maximum depth, minimum samples, and lack of improvement in splits.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 398,
        "QuestionDesc": "Which ensemble method improves Decision Tree performance?",
        "QuestionOptions": [
            "Random Forest",
            "K-Means Clustering",
            "Principal Component Analysis",
            "Linear Regression"
        ],
        "QuestionAnswer": "Random Forest",
        "QuestionReason": "Random Forest combines multiple decision trees to improve performance and reduce overfitting.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 399,
        "QuestionDesc": "Which disadvantage is associated with Decision Trees?",
        "QuestionOptions": [
            "High bias",
            "Overfitting",
            "Difficult interpretation",
            "High computational cost"
        ],
        "QuestionAnswer": "Overfitting",
        "QuestionReason": "Decision Trees are prone to overfitting, especially when deep.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 400,
        "QuestionDesc": "What is pruning in Decision Trees?",
        "QuestionOptions": [
            "Removing redundant branches",
            "Splitting nodes further",
            "Expanding the tree",
            "Increasing computational complexity"
        ],
        "QuestionAnswer": "Removing redundant branches",
        "QuestionReason": "Pruning removes unnecessary parts of the tree to improve generalization.",
        "QuestionTopic": "Decision Tree"
    },
    {
        "QuestionId": 401,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 402,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 403,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 404,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 405,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 406,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 407,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 408,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 409,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 410,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 411,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 412,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 413,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 414,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 415,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 416,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 417,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 418,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 419,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 420,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 421,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 422,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 423,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 424,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 425,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 426,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 427,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 428,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 429,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 430,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 431,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 432,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 433,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 434,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 435,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 436,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 437,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 438,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 439,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 440,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 441,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 442,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 443,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 444,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 445,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 446,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 447,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 448,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 449,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 450,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 451,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 452,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 453,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 454,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 455,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 456,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 457,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 458,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 459,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 460,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 461,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 462,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 463,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 464,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 465,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 466,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 467,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 468,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 469,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 470,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 471,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 472,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 473,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 474,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 475,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 476,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 477,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 478,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 479,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 480,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 481,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 482,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 483,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 484,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 485,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 486,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 487,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 488,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 489,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 490,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 491,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 492,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 493,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 494,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 495,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 496,
        "QuestionDesc": "What is the main purpose of a Random Forest model?",
        "QuestionOptions": [
            "To build a single decision tree",
            "To combine multiple decision trees for better accuracy",
            "To perform clustering",
            "To reduce dataset size"
        ],
        "QuestionAnswer": "To combine multiple decision trees for better accuracy",
        "QuestionReason": "Random Forest is an ensemble method that aggregates multiple decision trees to improve accuracy and reduce overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 497,
        "QuestionDesc": "Which of the following techniques is used in Random Forest to ensure diversity among trees?",
        "QuestionOptions": [
            "Boosting",
            "Bagging",
            "Gradient Descent",
            "Regularization"
        ],
        "QuestionAnswer": "Bagging",
        "QuestionReason": "Bagging (Bootstrap Aggregating) ensures diversity by training each decision tree on a different random subset of the dataset.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 498,
        "QuestionDesc": "What is the advantage of using Random Forest over a single decision tree?",
        "QuestionOptions": [
            "Higher interpretability",
            "Less computational power required",
            "Less prone to overfitting",
            "Always provides 100% accuracy"
        ],
        "QuestionAnswer": "Less prone to overfitting",
        "QuestionReason": "Random Forest reduces overfitting by averaging multiple decision trees, making the model more generalized.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 499,
        "QuestionDesc": "How does Random Forest handle missing values?",
        "QuestionOptions": [
            "Removes missing data points",
            "Uses mean imputation",
            "Can handle missing values internally",
            "Ignores missing values"
        ],
        "QuestionAnswer": "Can handle missing values internally",
        "QuestionReason": "Random Forest can handle missing values by using surrogate splits or by averaging predictions from multiple trees.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 500,
        "QuestionDesc": "What is the function of Bootstrap Aggregation (Bagging) in Random Forest?",
        "QuestionOptions": [
            "To improve interpretability",
            "To create diverse trees by sampling data with replacement",
            "To reduce the number of trees",
            "To increase bias"
        ],
        "QuestionAnswer": "To create diverse trees by sampling data with replacement",
        "QuestionReason": "Bagging allows each tree in the Random Forest to be trained on a different subset of the dataset, reducing overfitting.",
        "QuestionTopic": "Random Forest"
    },
    {
        "QuestionId": 501,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 502,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 503,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 504,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 505,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 506,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 507,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 508,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 509,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 510,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 511,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 512,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 513,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 514,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 515,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 516,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 517,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 518,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 519,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 520,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 521,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 522,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 523,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 524,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 525,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 526,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 527,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 528,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 529,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 530,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 531,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 532,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 533,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 534,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 535,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 536,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 537,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 538,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 539,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 540,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 541,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 542,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 543,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 544,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 545,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 546,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 547,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 548,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 549,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 550,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 551,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 552,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 553,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 554,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 555,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 556,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 557,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 558,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 559,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 560,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 561,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 562,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 563,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 564,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 565,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 566,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 567,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 568,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 569,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 570,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 571,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 572,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 573,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 574,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 575,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 576,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 577,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 578,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 579,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 580,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 581,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 582,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 583,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 584,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 585,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 586,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 587,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 588,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 589,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 590,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 591,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 592,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 593,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 594,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 595,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 596,
        "QuestionDesc": "What is the primary objective of a Support Vector Machine (SVM)?",
        "QuestionOptions": [
            "To minimize classification error by reducing bias",
            "To maximize the margin between classes",
            "To cluster similar data points together",
            "To create multiple decision trees"
        ],
        "QuestionAnswer": "To maximize the margin between classes",
        "QuestionReason": "SVM aims to find the hyperplane that maximizes the margin between different classes for better generalization.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 597,
        "QuestionDesc": "Which data points influence the position of the SVM hyperplane?",
        "QuestionOptions": [
            "All data points",
            "Support vectors",
            "Only outliers",
            "Randomly selected points"
        ],
        "QuestionAnswer": "Support vectors",
        "QuestionReason": "Support vectors are the closest data points to the hyperplane and determine its optimal placement.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 598,
        "QuestionDesc": "What role does the kernel function play in SVM?",
        "QuestionOptions": [
            "It reduces the number of features in the dataset",
            "It helps in visualizing data distributions",
            "It transforms non-linearly separable data into a higher-dimensional space",
            "It normalizes feature values"
        ],
        "QuestionAnswer": "It transforms non-linearly separable data into a higher-dimensional space",
        "QuestionReason": "Kernels like RBF and polynomial allow SVM to handle complex data distributions by mapping them to higher dimensions.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 599,
        "QuestionDesc": "Which of the following is NOT a commonly used SVM kernel function?",
        "QuestionOptions": [
            "Polynomial kernel",
            "Radial Basis Function (RBF) kernel",
            "Sigmoid kernel",
            "Mean kernel"
        ],
        "QuestionAnswer": "Mean kernel",
        "QuestionReason": "Mean kernel is not a standard SVM kernel, whereas polynomial, RBF, and sigmoid are commonly used.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 600,
        "QuestionDesc": "What does the regularization parameter (C) control in SVM?",
        "QuestionOptions": [
            "The number of support vectors",
            "The trade-off between margin maximization and classification error",
            "The number of kernel functions used",
            "The dimensionality of the feature space"
        ],
        "QuestionAnswer": "The trade-off between margin maximization and classification error",
        "QuestionReason": "A higher C value leads to lower bias but may cause overfitting, while a lower C results in a larger margin with some misclassifications.",
        "QuestionTopic": "Support Vector Machine"
    },
    {
        "QuestionId": 601,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 602,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 603,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 604,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 605,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 606,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 607,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 608,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 609,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 610,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 611,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 612,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 613,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 614,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 615,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 616,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 617,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 618,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 619,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 620,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 621,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 622,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 623,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 624,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 625,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 626,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 627,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 628,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 629,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 630,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 631,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 632,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 633,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 634,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 635,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 636,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 637,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 638,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 639,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 640,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 641,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 642,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 643,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 644,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 645,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 646,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 647,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 648,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 649,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 650,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 651,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 652,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 653,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 654,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 655,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 656,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 657,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 658,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 659,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 660,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 661,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 662,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 663,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 664,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 665,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 666,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 667,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 668,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 669,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 670,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 671,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 672,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 673,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 674,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 675,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 676,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 677,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 678,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 679,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 680,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 681,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 682,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 683,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 684,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 685,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 686,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 687,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 688,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 689,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 690,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 691,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 692,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 693,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 694,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 695,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 696,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 697,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 698,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 699,
        "QuestionDesc": "What assumption does Naive Bayes make about feature independence?",
        "QuestionOptions": [
            "Features are dependent on each other",
            "Features are conditionally independent given the class",
            "All features have equal importance",
            "Feature dependence is learned during training"
        ],
        "QuestionAnswer": "Features are conditionally independent given the class",
        "QuestionReason": "Naive Bayes assumes that features are independent of each other given the class label, making it 'naive'.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 700,
        "QuestionDesc": "Which type of Naive Bayes is best suited for text classification?",
        "QuestionOptions": [
            "Gaussian Naive Bayes",
            "Multinomial Naive Bayes",
            "Bernoulli Naive Bayes",
            "Bayesian Network"
        ],
        "QuestionAnswer": "Multinomial Naive Bayes",
        "QuestionReason": "Multinomial Naive Bayes is used for text data where features represent word frequencies or counts.",
        "QuestionTopic": "Naive Bayes"
    },
    {
        "QuestionId": 701,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 702,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 703,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 704,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 705,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 706,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 707,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 708,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 709,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 710,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 711,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 712,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 713,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 714,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 715,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 716,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 717,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 718,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 719,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 720,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 721,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 722,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 723,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 724,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 725,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 726,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 727,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 728,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 729,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 730,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 731,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 732,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 733,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 734,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 735,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 736,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 737,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 738,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 739,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 740,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 741,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 742,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 743,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 744,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 745,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 746,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 747,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 748,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 749,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 750,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 751,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 752,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 753,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 754,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 755,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 756,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 757,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 758,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 759,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 760,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 761,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 762,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 763,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 764,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 765,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 766,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 767,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 768,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 769,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 770,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 771,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 772,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 773,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 774,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 775,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 776,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 777,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 778,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 779,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 780,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 781,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 782,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 783,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 784,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 785,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 786,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 787,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 788,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 789,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 790,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 791,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 792,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 793,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 794,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 795,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 796,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 797,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 798,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 799,
        "QuestionDesc": "How does K-Nearest Neighbors (KNN) classify a new data point?",
        "QuestionOptions": [
            "By creating a decision boundary",
            "By assigning it to the majority class among its k nearest neighbors",
            "By using probability distributions",
            "By training multiple decision trees"
        ],
        "QuestionAnswer": "By assigning it to the majority class among its k nearest neighbors",
        "QuestionReason": "KNN assigns a data point to the class that appears most frequently among its k nearest neighbors.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 800,
        "QuestionDesc": "What happens if the value of k in KNN is too small?",
        "QuestionOptions": [
            "The model becomes more generalized",
            "The model becomes more sensitive to noise",
            "The model ignores the distance metric",
            "The model requires feature scaling"
        ],
        "QuestionAnswer": "The model becomes more sensitive to noise",
        "QuestionReason": "A small k value leads to high variance, making the model more prone to overfitting and noise in the data.",
        "QuestionTopic": "K-Nearest Neighbours"
    },
    {
        "QuestionId": 801,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 802,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 803,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 804,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 805,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 806,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 807,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 808,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 809,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 810,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 811,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 812,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 813,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 814,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 815,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 816,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 817,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 818,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 819,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 820,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 821,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 822,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 823,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 824,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 825,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 826,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 827,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 828,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 829,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 830,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 831,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 832,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 833,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 834,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 835,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 836,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 837,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 838,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 839,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 840,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 841,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 842,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 843,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 844,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 845,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 846,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 847,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 848,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 849,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 850,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 851,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 852,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 853,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 854,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 855,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 856,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 857,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 858,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 859,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 860,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 861,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 862,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 863,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 864,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 865,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 866,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 867,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 868,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 869,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 870,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 871,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 872,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 873,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 874,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 875,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 876,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 877,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 878,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 879,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 880,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 881,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 882,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 883,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 884,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 885,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 886,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 887,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 888,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 889,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 890,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 891,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 892,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 893,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 894,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 895,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 896,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 897,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 898,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 899,
        "QuestionDesc": "What is the primary purpose of Ridge Regression?",
        "QuestionOptions": [
            "To reduce the effect of correlated features",
            "To increase the variance of model predictions",
            "To maximize the coefficients",
            "To remove irrelevant features"
        ],
        "QuestionAnswer": "To reduce the effect of correlated features",
        "QuestionReason": "Ridge Regression adds an L2 penalty to prevent large coefficients, which helps in handling multicollinearity.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 900,
        "QuestionDesc": "Which term does Ridge Regression add to the cost function?",
        "QuestionOptions": [
            "L1 norm of coefficients",
            "Sum of squared coefficients",
            "Entropy of features",
            "Feature interactions"
        ],
        "QuestionAnswer": "Sum of squared coefficients",
        "QuestionReason": "Ridge Regression adds the L2 regularization term, which is the sum of squared coefficients, to the cost function.",
        "QuestionTopic": "Ridge Regression"
    },
    {
        "QuestionId": 901,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 902,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 903,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 904,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 905,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 906,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 907,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 908,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 909,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 910,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 911,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 912,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 913,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 914,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 915,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 916,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 917,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 918,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 919,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 920,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 921,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 922,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 923,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 924,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 925,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 926,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 927,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 928,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 929,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 930,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 931,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 932,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 933,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 934,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 935,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 936,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 937,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 938,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 939,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 940,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 941,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 942,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 943,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 944,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 945,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 946,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 947,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 948,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 949,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 950,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 951,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 952,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 953,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 954,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 955,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 956,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 957,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 958,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 959,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 960,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 961,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 962,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 963,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 964,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 965,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 966,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 967,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 968,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 969,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 970,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 971,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 972,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 973,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 974,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 975,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 976,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 977,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 978,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 979,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 980,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 981,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 982,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 983,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 984,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 985,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 986,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 987,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 988,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 989,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 990,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 991,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 992,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 993,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 994,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 995,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 996,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 997,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 998,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 999,
        "QuestionDesc": "What is the primary advantage of Lasso Regression over standard Linear Regression?",
        "QuestionOptions": [
            "Reduces multicollinearity",
            "Automatically selects important features",
            "Performs non-linear transformation",
            "Increases model complexity"
        ],
        "QuestionAnswer": "Automatically selects important features",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression applies L1 regularization, shrinking some coefficients to zero, thereby performing automatic feature selection."
    },
    {
        "QuestionId": 1000,
        "QuestionDesc": "Which type of regularization is used in Lasso Regression?",
        "QuestionOptions": [
            "L1",
            "L2",
            "Elastic Net",
            "Ridge"
        ],
        "QuestionAnswer": "L1",
        "QuestionTopic": "Lasso Regression",
        "QuestionReason": "Lasso Regression uses L1 regularization, which penalizes the absolute values of the coefficients."
    },
    {
        "QuestionId": 1001,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1002,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1003,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1004,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1005,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1006,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1007,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1008,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1009,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1010,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1011,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1012,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1013,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1014,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1015,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1016,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1017,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1018,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1019,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1020,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1021,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1022,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1023,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1024,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1025,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1026,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1027,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1028,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1029,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1030,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1031,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1032,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1033,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1034,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1035,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1036,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1037,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1038,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1039,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1040,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1041,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1042,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1043,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1044,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1045,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1046,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1047,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1048,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1049,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1050,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1051,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1052,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1053,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1054,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1055,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1056,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1057,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1058,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1059,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1060,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1061,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1062,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1063,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1064,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1065,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1066,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1067,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1068,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1069,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1070,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1071,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1072,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1073,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1074,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1075,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1076,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1077,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1078,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1079,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1080,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1081,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1082,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1083,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1084,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1085,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1086,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1087,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1088,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1089,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1090,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1091,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1092,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1093,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1094,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1095,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1096,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1097,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1098,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1099,
        "QuestionDesc": "What is the key assumption in K-Means clustering?",
        "QuestionOptions": [
            "Clusters are spherical and evenly sized",
            "Data follows a normal distribution",
            "All features are categorical",
            "The number of clusters is unknown"
        ],
        "QuestionAnswer": "Clusters are spherical and evenly sized",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means assumes clusters are convex, spherical, and of similar size for best performance."
    },
    {
        "QuestionId": 1100,
        "QuestionDesc": "Which method is commonly used to initialize centroids in K-Means?",
        "QuestionOptions": [
            "Random initialization",
            "K-Means++",
            "Hierarchical clustering",
            "PCA"
        ],
        "QuestionAnswer": "K-Means++",
        "QuestionTopic": "K Means",
        "QuestionReason": "K-Means++ improves the selection of initial centroids to reduce convergence time and avoid poor clustering."
    },
    {
        "QuestionId": 1101,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1102,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1103,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1104,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1105,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1106,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1107,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1108,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1109,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1110,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1111,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1112,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1113,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1114,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1115,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1116,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1117,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1118,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1119,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1120,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1121,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1122,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1123,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1124,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1125,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1126,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1127,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1128,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1129,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1130,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1131,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1132,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1133,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1134,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1135,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1136,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1137,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1138,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1139,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1140,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1141,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1142,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1143,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1144,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1145,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1146,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1147,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1148,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1149,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1150,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1151,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1152,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1153,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1154,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1155,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1156,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1157,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1158,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1159,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1160,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1161,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1162,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1163,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1164,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1165,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1166,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1167,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1168,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1169,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1170,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1171,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1172,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1173,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1174,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1175,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1176,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1177,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1178,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1179,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1180,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1181,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1182,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1183,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1184,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1185,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1186,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1187,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1188,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1189,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1190,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1191,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1192,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1193,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1194,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1195,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1196,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1197,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1198,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1199,
        "QuestionDesc": "How does K-Medoids differ from K-Means?",
        "QuestionOptions": [
            "It selects actual data points as cluster centers",
            "It is used only for large datasets",
            "It does not require the number of clusters",
            "It does not require distance metrics"
        ],
        "QuestionAnswer": "It selects actual data points as cluster centers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Unlike K-Means, K-Medoids chooses actual data points (medoids) as cluster centers, making it more robust to outliers."
    },
    {
        "QuestionId": 1200,
        "QuestionDesc": "What is a key advantage of K-Medoids over K-Means?",
        "QuestionOptions": [
            "Less sensitive to outliers",
            "Requires fewer iterations",
            "Works only with numeric data",
            "Does not require defining K in advance"
        ],
        "QuestionAnswer": "Less sensitive to outliers",
        "QuestionTopic": "K Medoids",
        "QuestionReason": "Since K-Medoids selects actual data points as centers, it is more robust to outliers compared to K-Means."
    },
    {
        "QuestionId": 1201,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1202,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1203,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1204,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1205,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1206,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1207,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1208,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1209,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1210,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1211,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1212,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1213,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1214,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1215,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1216,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1217,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1218,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1219,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1220,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1221,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1222,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1223,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1224,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1225,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1226,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1227,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1228,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1229,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1230,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1231,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1232,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1233,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1234,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1235,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1236,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1237,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1238,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1239,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1240,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1241,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1242,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1243,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1244,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1245,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1246,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1247,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1248,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1249,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1250,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1251,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1252,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1253,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1254,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1255,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1256,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1257,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1258,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1259,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1260,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1261,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1262,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1263,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1264,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1265,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1266,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1267,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1268,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1269,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1270,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1271,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1272,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1273,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1274,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1275,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1276,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1277,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1278,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1279,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1280,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1281,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1282,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1283,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1284,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1285,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1286,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1287,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1288,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1289,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1290,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1291,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1292,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1293,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1294,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1295,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1296,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1297,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1298,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1299,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1300,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1301,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1302,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1303,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1304,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1305,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1306,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1307,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1308,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1309,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1310,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1311,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1312,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1313,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1314,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1315,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1316,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1317,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1318,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1319,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1320,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1321,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1322,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1323,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1324,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1325,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1326,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1327,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1328,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1329,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1330,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1331,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1332,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1333,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1334,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1335,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1336,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1337,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1338,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1339,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1340,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1341,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1342,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1343,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1344,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1345,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1346,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1347,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1348,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1349,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1350,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1351,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1352,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1353,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1354,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1355,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1356,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1357,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1358,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1359,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1360,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1361,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1362,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1363,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1364,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1365,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1366,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1367,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1368,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1369,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1370,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1371,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1372,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1373,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1374,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1375,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1376,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1377,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1378,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1379,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1380,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1381,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1382,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1383,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1384,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1385,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1386,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1387,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1388,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1389,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1390,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1391,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1392,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1393,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1394,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1395,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1396,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1397,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1398,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1399,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1400,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1401,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1402,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1403,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1404,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1405,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1406,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1407,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1408,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1409,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1410,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1411,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1412,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1413,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1414,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1415,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1416,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1417,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1418,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1419,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1420,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1421,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1422,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1423,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1424,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1425,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1426,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1427,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1428,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1429,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1430,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1431,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1432,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1433,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1434,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1435,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1436,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1437,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1438,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1439,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1440,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1441,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1442,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1443,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1444,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1445,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1446,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1447,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1448,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1449,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1450,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1451,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1452,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1453,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1454,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1455,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1456,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1457,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1458,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1459,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1460,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1461,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1462,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1463,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1464,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1465,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1466,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1467,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1468,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1469,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1470,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1471,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1472,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1473,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1474,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1475,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1476,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1477,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1478,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1479,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1480,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1481,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1482,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1483,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1484,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1485,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1486,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1487,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1488,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1489,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1490,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1491,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1492,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1493,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1494,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    },
    {
        "QuestionId": 1495,
        "QuestionDesc": "What is the primary goal of supervised learning?",
        "QuestionOptions": [
            "To find hidden patterns in data without labels",
            "To learn from labeled data and make predictions",
            "To group similar data points together",
            "To reduce the dimensionality of data"
        ],
        "QuestionAnswer": "To learn from labeled data and make predictions",
        "QuestionTopic": "General",
        "QuestionReason": "Supervised learning algorithms learn from labeled data to make accurate predictions or classifications."
    },
    {
        "QuestionId": 1496,
        "QuestionDesc": "Which of the following is an unsupervised learning algorithm?",
        "QuestionOptions": [
            "Linear Regression",
            "Decision Tree",
            "K-Means Clustering",
            "Logistic Regression"
        ],
        "QuestionAnswer": "K-Means Clustering",
        "QuestionTopic": "General",
        "QuestionReason": "K-Means Clustering is an unsupervised algorithm used for grouping similar data points without predefined labels."
    },
    {
        "QuestionId": 1497,
        "QuestionDesc": "Which evaluation metric is most commonly used for classification tasks?",
        "QuestionOptions": [
            "Mean Squared Error",
            "R-Squared",
            "Accuracy",
            "Root Mean Squared Error"
        ],
        "QuestionAnswer": "Accuracy",
        "QuestionTopic": "General",
        "QuestionReason": "Accuracy is widely used in classification to measure the percentage of correctly classified instances."
    },
    {
        "QuestionId": 1498,
        "QuestionDesc": "What is overfitting in machine learning?",
        "QuestionOptions": [
            "When the model performs well on both training and test data",
            "When the model memorizes training data but fails on new data",
            "When the model ignores important features",
            "When the model underperforms on both training and test data"
        ],
        "QuestionAnswer": "When the model memorizes training data but fails on new data",
        "QuestionTopic": "General",
        "QuestionReason": "Overfitting occurs when a model learns patterns specific to training data but does not generalize well to unseen data."
    },
    {
        "QuestionId": 1499,
        "QuestionDesc": "Which technique can be used to reduce overfitting?",
        "QuestionOptions": [
            "Increasing the complexity of the model",
            "Using dropout in neural networks",
            "Training on fewer data samples",
            "Using a very small learning rate"
        ],
        "QuestionAnswer": "Using dropout in neural networks",
        "QuestionTopic": "General",
        "QuestionReason": "Dropout randomly disables neurons during training to prevent overfitting and improve generalization."
    },
    {
        "QuestionId": 1500,
        "QuestionDesc": "What is the purpose of cross-validation in machine learning?",
        "QuestionOptions": [
            "To improve model accuracy on the training data",
            "To check the performance of the model on unseen data",
            "To reduce the number of features",
            "To increase the training dataset size"
        ],
        "QuestionAnswer": "To check the performance of the model on unseen data",
        "QuestionTopic": "General",
        "QuestionReason": "Cross-validation helps evaluate how well a model generalizes to unseen data by splitting data into training and validation sets."
    }
]